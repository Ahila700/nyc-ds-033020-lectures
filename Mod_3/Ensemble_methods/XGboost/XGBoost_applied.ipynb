{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running an XGboosted Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the appropriate packages\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, f1_score, roc_auc_score\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data and split data to be used in the models\n",
    "titanic = pd.read_csv('cleaned_titanic.csv', index_col='PassengerId')\n",
    "\n",
    "# Create matrix of features\n",
    "X = titanic.drop('Survived', axis = 1) # grabs everything else but 'Survived'\n",
    "\n",
    "# Create target variable\n",
    "y = titanic['Survived'] # y is the column we're trying to predict\n",
    "\n",
    "# Create a list of the features being used in the \n",
    "feature_cols = X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up testing and training sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=23)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost's hyperparameters\n",
    "\n",
    "At this point, before building the model, you should be aware of the tuning parameters that XGBoost provides. Well, there are a plethora of tuning parameters for tree-based learners in XGBoost and you can read all about them [here](https://xgboost.readthedocs.io/en/latest/parameter.html#general-parameters). But the most common ones that you should know are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall parameters have been divided into 3 categories by XGBoost authors:\n",
    "\n",
    "- **General Parameters:** Guide the overall functioning\n",
    "- **Booster Parameters:** Guide the individual booster (tree/regression) at each step\n",
    "- **Learning Task Parameters:** Guide the optimization performed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General Parameters\n",
    "These define the overall functionality of XGBoost.\n",
    "\n",
    "- **booster** [default=gbtree]\n",
    "Select the type of model to run at each iteration. It has 2 options:\n",
    "    - gbtree: tree-based models\n",
    "    - gblinear: linear models\n",
    "    \n",
    "- **silent** [default=0]:\n",
    "Silent mode is activated is set to 1, i.e. no running messages will be printed. It’s generally good to keep it 0 as the messages might help in understanding the model.\n",
    "\n",
    "- **nthread**  [default to maximum number of threads available if not set]\n",
    "This is used for parallel processing and number of cores in the system should be entered. If you wish to run on all cores, value should not be entered and algorithm will detect automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Booster Parameters\n",
    "Though there are 2 types of boosters, we’ll consider only tree booster here because it always outperforms the linear booster and thus the latter is rarely used.\n",
    "\n",
    "- **eta [default=0.3]**\n",
    "    - Analogous to learning rate in GBM\n",
    "    - Makes the model more robust by shrinking the weights on each step\n",
    "    - Typical final values to be used: 0.01-0.2\n",
    "- **min_child_weight [default=1]**\n",
    "    - Defines the minimum sum of weights of all observations required in a child.\n",
    "    - This is similar to min_child_leaf in GBM but not exactly. This refers to min “sum of weights” of observations while GBM has min “number of observations”.\n",
    "    - Used to control over-fitting. Higher values prevent a model from learning relations which might be highly specific to the particular sample selected for a tree.\n",
    "    - Too high values can lead to under-fitting hence, it should be tuned using CV.\n",
    "- **max_depth [default=6]**\n",
    "    - The maximum depth of a tree, same as GBM.\n",
    "    - Used to control over-fitting as higher depth will allow model to learn relations very specific to a particular sample.\n",
    "    - Should be tuned using CV.\n",
    "    - Typical values: 3-10\n",
    "- **max_leaf_nodes**\n",
    "    - The maximum number of terminal nodes or leaves in a tree.\n",
    "    - Can be defined in place of max_depth. Since binary trees are created, a depth of ‘n’ would produce a maximum of 2^n leaves.\n",
    "    - If this is defined, GBM will ignore max_depth.\n",
    "- **gamma [default=0]**\n",
    "    - A node is split only when the resulting split gives a positive reduction in the loss function. Gamma specifies the minimum loss reduction required to make a split.\n",
    "    - Makes the algorithm conservative. The values can vary depending on the loss function and should be tuned.\n",
    "- **max_delta_step [default=0]**\n",
    "    - In maximum delta step we allow each tree’s weight estimation to be. If the value is set to 0, it means there is no constraint. If it is set to a positive value, it can help making the update step more conservative.\n",
    "    - Usually this parameter is not needed, but it might help in logistic regression when class is extremely imbalanced.\n",
    "    - This is generally not used but you can explore further if you wish.\n",
    "- **subsample [default=1]**\n",
    "    - Same as the subsample of GBM. Denotes the fraction of observations to be randomly samples for each tree.\n",
    "    - Lower values make the algorithm more conservative and prevents overfitting but too small values might lead to under-fitting.\n",
    "    - Typical values: 0.5-1\n",
    "- **colsample_bytree [default=1]**\n",
    "    - Similar to max_features in GBM. Denotes the fraction of columns to be randomly samples for each tree.\n",
    "    - Typical values: 0.5-1\n",
    "- **colsample_bylevel [default=1]**\n",
    "    - Denotes the subsample ratio of columns for each split, in each level.\n",
    "    - I don’t use this often because subsample and colsample_bytree will do the job for you. but you can explore further if you feel so.\n",
    "- **lambda [default=1]**\n",
    "    - L2 regularization term on weights (analogous to Ridge regression)\n",
    "    - This used to handle the regularization part of XGBoost. Though many data scientists don’t use it often, it should be explored to reduce overfitting.\n",
    "- **alpha [default=0]**\n",
    "    - L1 regularization term on weight (analogous to Lasso regression)\n",
    "    - Can be used in case of very high dimensionality so that the algorithm runs faster when implemented\n",
    "- **scale_pos_weight [default=1]**\n",
    "    - A value greater than 0 should be used in case of high class imbalance as it helps in faster convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Task Parameters\n",
    "\n",
    "These parameters are used to define the optimization objective the metric to be calculated at each step.\n",
    "\n",
    "- **objective [default=reg:linear]**\n",
    "    - This defines the loss function to be minimized. Mostly used values are:\n",
    "        - binary:logistic –logistic regression for binary classification, returns predicted probability (not class)\n",
    "        - multi:softmax –multiclass classification using the softmax objective, returns predicted class (not probabilities)\n",
    "                - you also need to set an additional num_class (number of classes) parameter defining the number of unique classes\n",
    "        - multi:softprob –same as softmax, but returns predicted probability of each data point belonging to each class.\n",
    "- **eval_metric [ default according to objective ]**\n",
    "    - The metric to be used for validation data.\n",
    "    - The default values are rmse for regression and error for classification.\n",
    "    - Typical values are:\n",
    "            - rmse – root mean square error\n",
    "            - mae – mean absolute error\n",
    "            - logloss – negative log-likelihood\n",
    "            - error – Binary classification error rate (0.5 threshold)\n",
    "            - merror – Multiclass classification error rate\n",
    "            - mlogloss – Multiclass logloss\n",
    "            - auc: Area under the curve\n",
    "- **seed [default=0]**\n",
    "    - The random number seed.\n",
    "    - Can be used for generating reproducible results and also for parameter tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Tuning with Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
       "              colsample_bynode=None, colsample_bytree=None, gamma=None,\n",
       "              gpu_id=None, importance_type='gain', interaction_constraints=None,\n",
       "              learning_rate=None, max_delta_step=None, max_depth=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              objective='binary:logistic', random_state=None, reg_alpha=None,\n",
       "              reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
       "              tree_method=None, validate_parameters=None, verbosity=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38245219347581555"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic['Survived'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_clf = xgb.XGBClassifier(objective ='binary:logistic', \n",
    "                           colsample_bytree = 0.3, \n",
    "                           subsample = 0.5,\n",
    "                           learning_rate = 0.1,\n",
    "                           max_depth = 4, \n",
    "                           alpha = 1, \n",
    "                           scale_pos_weight= titanic['Survived'].mean(),\n",
    "                           n_estimators = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(alpha=1, base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=4,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=10000, n_jobs=0, num_parallel_tree=1,\n",
       "              objective='binary:logistic', random_state=0, reg_alpha=1,\n",
       "              reg_lambda=1, scale_pos_weight=0.38245219347581555, subsample=0.5,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg_clf.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.802691\n",
      "F1: 0.676471\n"
     ]
    }
   ],
   "source": [
    "preds = xg_clf.predict(X_test)\n",
    "\n",
    "\n",
    "test_f1 = f1_score(y_test, preds)\n",
    "test_acc = accuracy_score(y_test, preds)\n",
    "\n",
    "print(\"Accuracy: %f\" % (test_acc))\n",
    "print(\"F1: %f\" % (test_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-fold Cross Validation using XGBoost\n",
    "In order to build more robust models, it is common to do a k-fold cross validation where all the entries in the original training dataset are used for both training as well as validation. XGBoost supports k-fold cross validation via the cv() method. All you have to do is specify the nfolds parameter, which is the number of cross validation sets you want to build. Also, it supports many other parameters (check out this link) like:\n",
    "\n",
    "- **num_boost_round**: denotes the number of trees you build (analogous to n_estimators)\n",
    "- **metrics:** tells the evaluation metrics to be watched during CV\n",
    "- **as_pandas**: to return the results in a pandas DataFrame.\n",
    "- **early_stopping_rounds: finishes training of the model early if the hold-out metric (\"rmse\" in our case) does not improve for a given number of rounds.\n",
    "- **seed**: for reproducibility of results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running your model, you will convert the dataset into an optimized data structure called Dmatrix that XGBoost supports and gives it acclaimed performance and efficiency gains. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params = {\"objective\":\"binary:logistic\",\n",
    "          'colsample_bytree': 0.3,\n",
    "          'learning_rate': 0.1,\n",
    "          'max_depth': 3, \n",
    "          'alpha': 1}\n",
    "\n",
    "cv_results = xgb.cv(dtrain=data_dmatrix, \n",
    "                    params=params, \n",
    "                    nfold=5,\n",
    "                    num_boost_round=500,\n",
    "                    early_stopping_rounds=5,\n",
    "                    metrics=\"logloss\", \n",
    "                    as_pandas=True, \n",
    "                    seed=123)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-logloss-mean</th>\n",
       "      <th>train-logloss-std</th>\n",
       "      <th>test-logloss-mean</th>\n",
       "      <th>test-logloss-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.659772</td>\n",
       "      <td>0.000848</td>\n",
       "      <td>0.660168</td>\n",
       "      <td>0.001479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.639740</td>\n",
       "      <td>0.006761</td>\n",
       "      <td>0.640184</td>\n",
       "      <td>0.010529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.627790</td>\n",
       "      <td>0.006071</td>\n",
       "      <td>0.629019</td>\n",
       "      <td>0.010380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.614868</td>\n",
       "      <td>0.010607</td>\n",
       "      <td>0.616503</td>\n",
       "      <td>0.015193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.599253</td>\n",
       "      <td>0.010672</td>\n",
       "      <td>0.601503</td>\n",
       "      <td>0.014880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>0.372616</td>\n",
       "      <td>0.009112</td>\n",
       "      <td>0.434313</td>\n",
       "      <td>0.038028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0.371920</td>\n",
       "      <td>0.009004</td>\n",
       "      <td>0.434456</td>\n",
       "      <td>0.038454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>0.371195</td>\n",
       "      <td>0.009250</td>\n",
       "      <td>0.434110</td>\n",
       "      <td>0.038592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>0.370798</td>\n",
       "      <td>0.009382</td>\n",
       "      <td>0.434108</td>\n",
       "      <td>0.038613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0.370584</td>\n",
       "      <td>0.009246</td>\n",
       "      <td>0.433982</td>\n",
       "      <td>0.038632</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>132 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     train-logloss-mean  train-logloss-std  test-logloss-mean  \\\n",
       "0              0.659772           0.000848           0.660168   \n",
       "1              0.639740           0.006761           0.640184   \n",
       "2              0.627790           0.006071           0.629019   \n",
       "3              0.614868           0.010607           0.616503   \n",
       "4              0.599253           0.010672           0.601503   \n",
       "..                  ...                ...                ...   \n",
       "127            0.372616           0.009112           0.434313   \n",
       "128            0.371920           0.009004           0.434456   \n",
       "129            0.371195           0.009250           0.434110   \n",
       "130            0.370798           0.009382           0.434108   \n",
       "131            0.370584           0.009246           0.433982   \n",
       "\n",
       "     test-logloss-std  \n",
       "0            0.001479  \n",
       "1            0.010529  \n",
       "2            0.010380  \n",
       "3            0.015193  \n",
       "4            0.014880  \n",
       "..                ...  \n",
       "127          0.038028  \n",
       "128          0.038454  \n",
       "129          0.038592  \n",
       "130          0.038613  \n",
       "131          0.038632  \n",
       "\n",
       "[132 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xgb.plot_importance(xg_clf)\n",
    "plt.rcParams['figure.figsize'] = [5, 5]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelfit(alg, dtrain, predictors, target, useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(dtrain[predictors].values, label=dtrain[target].values)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "            metrics='auc', early_stopping_rounds=early_stopping_rounds)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(dtrain[predictors], dtrain[target],eval_metric='auc')\n",
    "        \n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(dtrain[predictors])\n",
    "    dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n",
    "        \n",
    "    #Print model report:\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"Accuracy : %.4g\" % accuracy_score(dtrain[target].values, dtrain_predictions))\n",
    "    print(\"AUC Score (Train): %f\" % roc_auc_score(dtrain[target], dtrain_predprob))\n",
    "\n",
    "    return alg\n",
    "#     feat_imp = pd.Series(alg.get_booster().get_fscore())\n",
    "#     feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "#     plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train  = pd.concat([X_train, y_train], axis=1)\n",
    "target = 'Survived'\n",
    "IDcol = 'PassengerId'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>youngin</th>\n",
       "      <th>male</th>\n",
       "      <th>Q</th>\n",
       "      <th>S</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>3</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>3</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34.3750</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>3</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641</th>\n",
       "      <td>3</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8542</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Pclass   Age  SibSp  Parch     Fare  youngin  male  Q  S  \\\n",
       "PassengerId                                                             \n",
       "740               3  24.0      0      0   7.8958        0     1  0  1   \n",
       "148               3   9.0      2      2  34.3750        1     0  0  1   \n",
       "876               3  15.0      0      0   7.2250        0     0  0  0   \n",
       "641               3  20.0      0      0   7.8542        0     1  0  1   \n",
       "885               3  25.0      0      0   7.0500        0     1  0  1   \n",
       "\n",
       "             Survived  \n",
       "PassengerId            \n",
       "740                 0  \n",
       "148                 0  \n",
       "876                 1  \n",
       "641                 0  \n",
       "885                 0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "Accuracy : 0.8919\n",
      "AUC Score (Train): 0.939281\n"
     ]
    }
   ],
   "source": [
    "#Choose all predictors except target & IDcols\n",
    "predictors = [x for x in train.columns if x not in [target, IDcol]]\n",
    "xgb1 = xgb.XGBClassifier(\n",
    " learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=3,\n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.6,\n",
    " colsample_bytree=0.3,\n",
    " objective= 'binary:logistic',\n",
    " nthread=4,\n",
    " scale_pos_weight=1,\n",
    " seed=27)\n",
    "\n",
    "alg = modelfit(xgb1, train, predictors, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.802691\n",
      "F1: 0.702703\n"
     ]
    }
   ],
   "source": [
    "preds = alg.predict(X_test)\n",
    "\n",
    "\n",
    "test_f1 = f1_score(y_test, preds)\n",
    "test_acc = accuracy_score(y_test, preds)\n",
    "\n",
    "print(\"Accuracy: %f\" % (test_acc))\n",
    "print(\"F1: %f\" % (test_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining XGBoost with GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import RandomizedSearchCV, KFold\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf_xgb = xgb.XGBClassifier(objective = 'binary:logistic')\n",
    "param_dist = {'n_estimators': [100,300,500],\n",
    "              'learning_rate': [0.1,0.07,0.05,0.03,0.01],\n",
    "              'max_depth': [3, 4, 5, 6, 7],\n",
    "              'colsample_bytree': [0.5,0.45,0.4],\n",
    "              'min_child_weight': [1, 2, 3]\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate the Gridsearch model\n",
    "gsearch1 = GridSearchCV(\n",
    "    estimator = clf_xgb,\n",
    "    param_grid = param_dist, \n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    iid=False, \n",
    "    cv=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 675 candidates, totalling 3375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    3.7s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    7.4s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:   14.0s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:   22.9s\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:   35.0s\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed:   50.3s\n",
      "[Parallel(n_jobs=-1)]: Done 2434 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 3184 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 3375 out of 3375 | elapsed:  1.5min finished\n",
      "/Users/antoniohila/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:823: FutureWarning: The parameter 'iid' is deprecated in 0.22 and will be removed in 0.24.\n",
      "  \"removed in 0.24.\", FutureWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                     colsample_bylevel=None,\n",
       "                                     colsample_bynode=None,\n",
       "                                     colsample_bytree=None, gamma=None,\n",
       "                                     gpu_id=None, importance_type='gain',\n",
       "                                     interaction_constraints=None,\n",
       "                                     learning_rate=None, max_delta_step=None,\n",
       "                                     max_depth=None, min_child_weight=None,\n",
       "                                     missing=nan, monotone_constraints=None,\n",
       "                                     n_estim...\n",
       "                                     reg_lambda=None, scale_pos_weight=None,\n",
       "                                     subsample=None, tree_method=None,\n",
       "                                     validate_parameters=None, verbosity=None),\n",
       "             iid=False, n_jobs=-1,\n",
       "             param_grid={'colsample_bytree': [0.5, 0.45, 0.4],\n",
       "                         'learning_rate': [0.1, 0.07, 0.05, 0.03, 0.01],\n",
       "                         'max_depth': [3, 4, 5, 6, 7],\n",
       "                         'min_child_weight': [1, 2, 3],\n",
       "                         'n_estimators': [100, 300, 500]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='f1', verbose=1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.fit(train[predictors],train[target])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.06041813, 0.14215569, 0.22778344, 0.05189166, 0.14606776,\n",
       "        0.22195916, 0.05285897, 0.14558301, 0.24546657, 0.06340461,\n",
       "        0.16677976, 0.28628197, 0.06293898, 0.16042142, 0.26075563,\n",
       "        0.05977707, 0.1707746 , 0.30569072, 0.06813779, 0.18689961,\n",
       "        0.32791986, 0.06947851, 0.19082179, 0.30845604, 0.06701307,\n",
       "        0.1848362 , 0.29665241, 0.07204318, 0.22062907, 0.36148543,\n",
       "        0.07413068, 0.20895157, 0.33828487, 0.07398982, 0.19911456,\n",
       "        0.33281417, 0.0835228 , 0.23027148, 0.39878883, 0.08732524,\n",
       "        0.21830974, 0.34366698, 0.0757627 , 0.21799974, 0.35240126,\n",
       "        0.05292921, 0.1389946 , 0.23142166, 0.05191793, 0.14103918,\n",
       "        0.23189325, 0.05156374, 0.13689947, 0.2376123 , 0.06452646,\n",
       "        0.16844301, 0.27362719, 0.06019125, 0.16149082, 0.24344983,\n",
       "        0.06049471, 0.15254259, 0.28417072, 0.07167439, 0.19464393,\n",
       "        0.32822337, 0.06792002, 0.21150599, 0.31225657, 0.06479535,\n",
       "        0.20844245, 0.29995699, 0.07864118, 0.21982913, 0.36009459,\n",
       "        0.07360535, 0.24104714, 0.38884401, 0.08095889, 0.23176131,\n",
       "        0.37950716, 0.09210281, 0.26061845, 0.4291101 , 0.08739281,\n",
       "        0.26586385, 0.37930193, 0.0930882 , 0.2321888 , 0.38593979,\n",
       "        0.0552938 , 0.15158787, 0.25323715, 0.05662446, 0.15346365,\n",
       "        0.24098558, 0.05539322, 0.14947467, 0.24039574, 0.06273537,\n",
       "        0.17166262, 0.28412023, 0.06545677, 0.16921601, 0.27002831,\n",
       "        0.05837417, 0.16456614, 0.27566853, 0.0742094 , 0.20375094,\n",
       "        0.32308564, 0.07377372, 0.20489049, 0.324545  , 0.07011909,\n",
       "        0.20215669, 0.32432661, 0.07941399, 0.23204408, 0.35416565,\n",
       "        0.07533693, 0.21874485, 0.35118141, 0.07747827, 0.21660361,\n",
       "        0.35566716, 0.08842425, 0.24564357, 0.41418247, 0.08942242,\n",
       "        0.2445714 , 0.38985648, 0.08692522, 0.21956439, 0.36602921,\n",
       "        0.05614667, 0.1520452 , 0.24798512, 0.05634956, 0.15194297,\n",
       "        0.24246664, 0.05754542, 0.15672297, 0.23048754, 0.06530113,\n",
       "        0.17950854, 0.29331474, 0.06138573, 0.18262482, 0.28728838,\n",
       "        0.06408877, 0.17276125, 0.28373995, 0.07100401, 0.21259971,\n",
       "        0.34092479, 0.073592  , 0.20825348, 0.35236254, 0.07673912,\n",
       "        0.20760326, 0.33485155, 0.08594737, 0.24225907, 0.38302288,\n",
       "        0.07878942, 0.21566787, 0.37122784, 0.07907166, 0.22442756,\n",
       "        0.3674273 , 0.08970065, 0.26165099, 0.42456408, 0.08812342,\n",
       "        0.25550275, 0.4113214 , 0.08727031, 0.2316566 , 0.38825159,\n",
       "        0.05602312, 0.14887462, 0.25205164, 0.05359516, 0.1567668 ,\n",
       "        0.25188222, 0.05633364, 0.15308385, 0.24968143, 0.06548786,\n",
       "        0.19727602, 0.31249461, 0.06710062, 0.180619  , 0.29340758,\n",
       "        0.0615438 , 0.1794837 , 0.29554548, 0.07905793, 0.21999574,\n",
       "        0.35596213, 0.07696681, 0.21171689, 0.341927  , 0.07215757,\n",
       "        0.20941381, 0.34195085, 0.08128238, 0.24537883, 0.37889934,\n",
       "        0.08152456, 0.22915263, 0.37902217, 0.08362498, 0.22485776,\n",
       "        0.38021841, 0.09157863, 0.25652509, 0.43085599, 0.08707924,\n",
       "        0.24936662, 0.40931072, 0.08793154, 0.23466568, 0.41046095,\n",
       "        0.05377946, 0.15323968, 0.23772779, 0.05603719, 0.15259857,\n",
       "        0.25678983, 0.05728688, 0.1577723 , 0.25119681, 0.06619239,\n",
       "        0.18454003, 0.3040585 , 0.06465664, 0.18481369, 0.2834888 ,\n",
       "        0.06257815, 0.17567182, 0.29873362, 0.0777813 , 0.21548076,\n",
       "        0.3493959 , 0.07801704, 0.20751877, 0.35712457, 0.07415056,\n",
       "        0.21963983, 0.33532019, 0.08601213, 0.23804021, 0.38752079,\n",
       "        0.08373866, 0.22971311, 0.37748885, 0.07978148, 0.22218266,\n",
       "        0.36475654, 0.09104815, 0.27940888, 0.43048863, 0.09627676,\n",
       "        0.24213228, 0.38842678, 0.08307076, 0.23070798, 0.38653979,\n",
       "        0.05950184, 0.1523984 , 0.25214024, 0.05571918, 0.15938711,\n",
       "        0.254704  , 0.05541511, 0.15441418, 0.25901423, 0.0642252 ,\n",
       "        0.18140836, 0.29307766, 0.06795082, 0.17528944, 0.30138292,\n",
       "        0.06299458, 0.17596903, 0.2972136 , 0.07197132, 0.21452379,\n",
       "        0.34877954, 0.07617402, 0.20240917, 0.35123982, 0.07738638,\n",
       "        0.20415969, 0.34289141, 0.08580618, 0.23198652, 0.381249  ,\n",
       "        0.08199787, 0.23562055, 0.37817702, 0.08228598, 0.2279376 ,\n",
       "        0.37165399, 0.09314461, 0.25495176, 0.42847314, 0.08906951,\n",
       "        0.24207506, 0.38984618, 0.08011718, 0.24234958, 0.39365163,\n",
       "        0.06250081, 0.15087686, 0.2616622 , 0.06016798, 0.15566654,\n",
       "        0.25553718, 0.05442777, 0.1522326 , 0.25347633, 0.06533442,\n",
       "        0.18817043, 0.29919062, 0.06409516, 0.17499266, 0.31040535,\n",
       "        0.06771865, 0.18613205, 0.29373851, 0.07858081, 0.21547771,\n",
       "        0.35686035, 0.07601409, 0.21181617, 0.34474201, 0.07813735,\n",
       "        0.21510038, 0.32404475, 0.08286386, 0.22695518, 0.39473886,\n",
       "        0.07879658, 0.22894607, 0.38591247, 0.08432102, 0.22744255,\n",
       "        0.36778789, 0.0945169 , 0.26033163, 0.4230547 , 0.08707833,\n",
       "        0.23647075, 0.41071   , 0.08393998, 0.23916659, 0.41449242,\n",
       "        0.06119533, 0.16004977, 0.26353254, 0.05575242, 0.15174541,\n",
       "        0.25203032, 0.05788012, 0.15183711, 0.24667168, 0.06537876,\n",
       "        0.18519425, 0.28808765, 0.06294103, 0.17874103, 0.3021606 ,\n",
       "        0.0662478 , 0.18703837, 0.30479074, 0.07726798, 0.22026525,\n",
       "        0.3589344 , 0.07447882, 0.20715775, 0.35011039, 0.07318115,\n",
       "        0.21123915, 0.32987208, 0.08179779, 0.23133574, 0.39009051,\n",
       "        0.08302283, 0.23578916, 0.38835464, 0.08539524, 0.22962475,\n",
       "        0.37469983, 0.09390965, 0.26082201, 0.42230535, 0.08874598,\n",
       "        0.23269777, 0.41024327, 0.08673058, 0.24607811, 0.39534364,\n",
       "        0.05582824, 0.15830874, 0.24876056, 0.05841279, 0.15238438,\n",
       "        0.26274614, 0.06103587, 0.16111917, 0.25465341, 0.06955433,\n",
       "        0.17667632, 0.29454527, 0.06416087, 0.18363104, 0.30489855,\n",
       "        0.067241  , 0.18226423, 0.30378036, 0.07737784, 0.21103945,\n",
       "        0.35723085, 0.07626114, 0.21399498, 0.34918623, 0.07324123,\n",
       "        0.19995456, 0.33139763, 0.0785399 , 0.23777561, 0.39358473,\n",
       "        0.08116069, 0.23072958, 0.38353925, 0.07778864, 0.22396598,\n",
       "        0.3773912 , 0.09306383, 0.25773826, 0.40720134, 0.08547258,\n",
       "        0.23751421, 0.41566515, 0.08823738, 0.24320002, 0.40306139,\n",
       "        0.05239086, 0.14642029, 0.22874484, 0.04795923, 0.14044738,\n",
       "        0.22808471, 0.05037484, 0.13828382, 0.22717533, 0.05804725,\n",
       "        0.16365638, 0.25830283, 0.05716333, 0.16180649, 0.26506352,\n",
       "        0.05747876, 0.1618454 , 0.26855145, 0.07153363, 0.18161759,\n",
       "        0.30243335, 0.06760283, 0.17333741, 0.29914432, 0.06498637,\n",
       "        0.17715631, 0.2847415 , 0.06859217, 0.18797717, 0.31327896,\n",
       "        0.07254114, 0.19842887, 0.31088762, 0.07024012, 0.19148393,\n",
       "        0.31339388, 0.07843184, 0.21250429, 0.36938677, 0.08716455,\n",
       "        0.20448546, 0.3422977 , 0.08122878, 0.18753924, 0.31314559,\n",
       "        0.04798622, 0.14935179, 0.22214656, 0.04924736, 0.14221907,\n",
       "        0.22653923, 0.05250239, 0.13717661, 0.22668066, 0.05942225,\n",
       "        0.16454129, 0.27060838, 0.0546886 , 0.15865922, 0.26814456,\n",
       "        0.0586525 , 0.1573719 , 0.2513061 , 0.06356459, 0.17978287,\n",
       "        0.29561477, 0.06418118, 0.17642899, 0.29944367, 0.06700411,\n",
       "        0.19043431, 0.30149841, 0.07846208, 0.20215569, 0.33000255,\n",
       "        0.07180042, 0.18725123, 0.3118104 , 0.06421127, 0.186621  ,\n",
       "        0.30708776, 0.07520318, 0.21844716, 0.41108766, 0.08426838,\n",
       "        0.22133265, 0.34056702, 0.0729528 , 0.1990787 , 0.32353635,\n",
       "        0.05057445, 0.13926444, 0.21987181, 0.04821897, 0.12951622,\n",
       "        0.22094378, 0.04857392, 0.13907871, 0.223943  , 0.06052046,\n",
       "        0.16330986, 0.27544541, 0.06142302, 0.16715169, 0.27549882,\n",
       "        0.05769806, 0.16120005, 0.26237292, 0.06763973, 0.18144112,\n",
       "        0.30329723, 0.06504474, 0.16650567, 0.27844143, 0.06155453,\n",
       "        0.17370882, 0.28873768, 0.07190161, 0.19665871, 0.32443399,\n",
       "        0.07164278, 0.19532957, 0.31483588, 0.06782284, 0.18868642,\n",
       "        0.30935278, 0.07529821, 0.2127584 , 0.32991724, 0.07065301,\n",
       "        0.1998374 , 0.33309159, 0.07364955, 0.19886699, 0.32743778,\n",
       "        0.05264325, 0.14181156, 0.22878194, 0.05103259, 0.13822594,\n",
       "        0.2320816 , 0.05624027, 0.13652396, 0.22619472, 0.05665812,\n",
       "        0.16051149, 0.25210657, 0.0572197 , 0.15638218, 0.26408362,\n",
       "        0.05792723, 0.15685415, 0.26184134, 0.06277452, 0.18876996,\n",
       "        0.30085878, 0.06409669, 0.17778974, 0.28895555, 0.06669917,\n",
       "        0.17633324, 0.28986697, 0.06961002, 0.20049076, 0.30875316,\n",
       "        0.06529284, 0.184582  , 0.32368875, 0.06850142, 0.19000826,\n",
       "        0.30974259, 0.07963219, 0.21216178, 0.34992814, 0.07305269,\n",
       "        0.20550556, 0.33409176, 0.07365623, 0.19239101, 0.30462446,\n",
       "        0.04796309, 0.13204188, 0.22307258, 0.05072875, 0.1393599 ,\n",
       "        0.22430301, 0.05154843, 0.13923717, 0.2430234 , 0.06278281,\n",
       "        0.17716641, 0.27955036, 0.05425572, 0.153654  , 0.26008806,\n",
       "        0.05804801, 0.15887184, 0.25337882, 0.06437478, 0.17078395,\n",
       "        0.28764215, 0.06402941, 0.17773919, 0.29024677, 0.06412816,\n",
       "        0.18383741, 0.29363503, 0.07100167, 0.19433031, 0.32971864,\n",
       "        0.07026253, 0.18945312, 0.3224431 , 0.06874127, 0.18630586,\n",
       "        0.30001593, 0.07165475, 0.20530772, 0.36452689, 0.07003474,\n",
       "        0.19632745, 0.33900809, 0.07188516, 0.19803066, 0.24975014]),\n",
       " 'std_fit_time': array([0.00255862, 0.01470387, 0.01863976, 0.0038308 , 0.01581722,\n",
       "        0.01663237, 0.00305298, 0.00839568, 0.0079704 , 0.00476027,\n",
       "        0.00478039, 0.01975043, 0.00485329, 0.00342214, 0.00483998,\n",
       "        0.00405081, 0.01101112, 0.02923603, 0.00474265, 0.02050225,\n",
       "        0.03565123, 0.01102331, 0.01867437, 0.00444371, 0.00271693,\n",
       "        0.00522191, 0.02482948, 0.0045554 , 0.00878602, 0.00212049,\n",
       "        0.00239826, 0.00521883, 0.00626791, 0.00325385, 0.00170753,\n",
       "        0.00311498, 0.00469302, 0.00321834, 0.02280817, 0.00751788,\n",
       "        0.01391012, 0.00739517, 0.0027154 , 0.00318879, 0.00628024,\n",
       "        0.00322964, 0.00470591, 0.00746069, 0.00427824, 0.00554616,\n",
       "        0.00818055, 0.00235521, 0.00771646, 0.00529716, 0.00415017,\n",
       "        0.00191556, 0.00412906, 0.00474447, 0.00549949, 0.0121987 ,\n",
       "        0.00336277, 0.02453522, 0.02031219, 0.00252063, 0.00285499,\n",
       "        0.01310794, 0.00282771, 0.00217884, 0.00811412, 0.0034689 ,\n",
       "        0.06750912, 0.00651331, 0.01322089, 0.00339571, 0.01181756,\n",
       "        0.00167527, 0.00660691, 0.00847675, 0.00446366, 0.01446705,\n",
       "        0.0043541 , 0.00346277, 0.00367179, 0.01122812, 0.00508292,\n",
       "        0.04042039, 0.00150878, 0.00810235, 0.00541129, 0.00380571,\n",
       "        0.00373662, 0.00433336, 0.0132715 , 0.00357678, 0.00503352,\n",
       "        0.00142619, 0.00305275, 0.00172874, 0.02220923, 0.01062627,\n",
       "        0.01426533, 0.0173353 , 0.00374093, 0.00553316, 0.00967751,\n",
       "        0.0023579 , 0.00181063, 0.00510433, 0.00281028, 0.00636559,\n",
       "        0.00754249, 0.00471097, 0.00281381, 0.00345418, 0.00510518,\n",
       "        0.00528381, 0.01175178, 0.00395366, 0.00242134, 0.01971063,\n",
       "        0.00221361, 0.01900799, 0.00969641, 0.0055275 , 0.00726793,\n",
       "        0.02522256, 0.00707819, 0.00310451, 0.00638529, 0.00431727,\n",
       "        0.0034887 , 0.00559538, 0.00166921, 0.00276049, 0.01200256,\n",
       "        0.00529428, 0.005787  , 0.00261317, 0.00483837, 0.00182981,\n",
       "        0.00910201, 0.00328218, 0.01701572, 0.01095825, 0.00292538,\n",
       "        0.0055374 , 0.01119992, 0.00417172, 0.0155898 , 0.00147011,\n",
       "        0.00246352, 0.00260948, 0.01334225, 0.00359028, 0.00438514,\n",
       "        0.00709758, 0.00453862, 0.00446341, 0.00515711, 0.0054638 ,\n",
       "        0.0035306 , 0.00385073, 0.00456939, 0.00633892, 0.0109484 ,\n",
       "        0.00418596, 0.0023753 , 0.00688284, 0.00246481, 0.00426081,\n",
       "        0.00590995, 0.00238754, 0.00722235, 0.0091095 , 0.00144933,\n",
       "        0.00402655, 0.00990837, 0.00377104, 0.00438131, 0.00277302,\n",
       "        0.00356552, 0.00378901, 0.0092501 , 0.00376532, 0.00596908,\n",
       "        0.00226986, 0.00375861, 0.00145119, 0.00582833, 0.00236135,\n",
       "        0.0095188 , 0.0056944 , 0.00275334, 0.0048307 , 0.01932925,\n",
       "        0.00343495, 0.00404891, 0.02030042, 0.00138397, 0.0055989 ,\n",
       "        0.00330934, 0.00313262, 0.00456903, 0.00326257, 0.00141929,\n",
       "        0.00297737, 0.00602677, 0.0029977 , 0.03650282, 0.00733833,\n",
       "        0.00542739, 0.00393858, 0.0269414 , 0.0044171 , 0.01384941,\n",
       "        0.01608952, 0.00320732, 0.00407378, 0.01089265, 0.00385404,\n",
       "        0.00228053, 0.00300497, 0.00223718, 0.00635171, 0.00532499,\n",
       "        0.00345436, 0.00347625, 0.01366026, 0.00675531, 0.00669706,\n",
       "        0.00364467, 0.00498815, 0.00455314, 0.0037043 , 0.00438876,\n",
       "        0.00221683, 0.00418434, 0.0046853 , 0.00535079, 0.00318148,\n",
       "        0.00332319, 0.00412495, 0.00725809, 0.00385092, 0.00166491,\n",
       "        0.00529711, 0.00269369, 0.00375573, 0.00522495, 0.00408302,\n",
       "        0.00425926, 0.02165757, 0.0140278 , 0.00663302, 0.00704605,\n",
       "        0.00371869, 0.00358478, 0.0034781 , 0.00318373, 0.00315132,\n",
       "        0.00372714, 0.00427308, 0.04548522, 0.01389009, 0.00554553,\n",
       "        0.01129038, 0.00874452, 0.00461859, 0.01837101, 0.02063007,\n",
       "        0.00533912, 0.0103036 , 0.00410764, 0.0020445 , 0.00226378,\n",
       "        0.00173393, 0.00432695, 0.00334256, 0.02049314, 0.00140945,\n",
       "        0.00711385, 0.01867113, 0.00259932, 0.00754065, 0.00895737,\n",
       "        0.00263402, 0.01094698, 0.00377273, 0.00316427, 0.00561141,\n",
       "        0.00628447, 0.00274418, 0.01624091, 0.01702643, 0.00202427,\n",
       "        0.00423175, 0.00492405, 0.00520997, 0.00517055, 0.00622577,\n",
       "        0.00324479, 0.00435356, 0.00577587, 0.01068223, 0.00604237,\n",
       "        0.00209131, 0.00250088, 0.00129807, 0.00815361, 0.00327943,\n",
       "        0.01628637, 0.01327267, 0.00332108, 0.00382242, 0.02180226,\n",
       "        0.00801506, 0.01573835, 0.00610784, 0.00233817, 0.00562644,\n",
       "        0.01368807, 0.00343061, 0.01524824, 0.00963842, 0.00229663,\n",
       "        0.00503511, 0.00467912, 0.00268995, 0.00316066, 0.02675763,\n",
       "        0.00334851, 0.00174935, 0.00632773, 0.00467036, 0.00366303,\n",
       "        0.00250522, 0.0029288 , 0.00565587, 0.00494903, 0.00680358,\n",
       "        0.02681176, 0.00427931, 0.00906697, 0.00259479, 0.02970813,\n",
       "        0.00505414, 0.00512772, 0.00867809, 0.00403086, 0.00327552,\n",
       "        0.0148071 , 0.00291832, 0.00579521, 0.00646073, 0.00539649,\n",
       "        0.00923449, 0.05799367, 0.0021564 , 0.00762768, 0.00407406,\n",
       "        0.00456227, 0.01182592, 0.00472935, 0.00319799, 0.00499456,\n",
       "        0.00719148, 0.00282304, 0.00406321, 0.00195274, 0.0020086 ,\n",
       "        0.00442351, 0.00436735, 0.00356697, 0.01379246, 0.00274412,\n",
       "        0.00336957, 0.00709379, 0.00624279, 0.00562508, 0.00284606,\n",
       "        0.00606765, 0.00409558, 0.01647444, 0.0198471 , 0.00458449,\n",
       "        0.0038348 , 0.00279323, 0.00443465, 0.00423111, 0.00462913,\n",
       "        0.00435788, 0.00412036, 0.00534196, 0.00326087, 0.0056496 ,\n",
       "        0.00722671, 0.00319856, 0.00389475, 0.00173306, 0.00121532,\n",
       "        0.00465807, 0.01676653, 0.0026961 , 0.00304774, 0.00292468,\n",
       "        0.00164713, 0.0048671 , 0.01678957, 0.00385757, 0.01336091,\n",
       "        0.01869662, 0.00597094, 0.00564167, 0.00430788, 0.00406486,\n",
       "        0.00622357, 0.00540339, 0.00186995, 0.00468353, 0.00797729,\n",
       "        0.00342047, 0.00694948, 0.00135917, 0.00259955, 0.02167724,\n",
       "        0.00687627, 0.00304227, 0.0036905 , 0.00586763, 0.00590881,\n",
       "        0.00582608, 0.02560643, 0.00419607, 0.00856172, 0.00548555,\n",
       "        0.00466717, 0.00388877, 0.00399622, 0.00241469, 0.00460643,\n",
       "        0.01594864, 0.00634719, 0.00334124, 0.01039105, 0.00544884,\n",
       "        0.00990623, 0.00177928, 0.0040214 , 0.00712721, 0.00603155,\n",
       "        0.00307114, 0.03434742, 0.01783573, 0.00693948, 0.01160766,\n",
       "        0.00500972, 0.00303797, 0.005847  , 0.00290151, 0.00276813,\n",
       "        0.00425075, 0.002814  , 0.00269722, 0.00240528, 0.00334753,\n",
       "        0.00317064, 0.00240826, 0.00330526, 0.00274287, 0.00443688,\n",
       "        0.00322909, 0.00345134, 0.00434565, 0.00803014, 0.00441781,\n",
       "        0.00323947, 0.00355034, 0.00478612, 0.00189729, 0.00538796,\n",
       "        0.00337775, 0.02902899, 0.00475963, 0.00310714, 0.00784537,\n",
       "        0.00427809, 0.00458378, 0.00465859, 0.00485459, 0.00495926,\n",
       "        0.00673907, 0.00918277, 0.00863653, 0.00467502, 0.00370411,\n",
       "        0.00240373, 0.02348697, 0.00652578, 0.00193892, 0.00637836,\n",
       "        0.00231612, 0.00261402, 0.00625416, 0.00215224, 0.00538212,\n",
       "        0.00316632, 0.01308017, 0.00657149, 0.00398204, 0.00279708,\n",
       "        0.00377594, 0.00316514, 0.00827872, 0.00424639, 0.01390758,\n",
       "        0.00465362, 0.00277149, 0.00401782, 0.00592473, 0.00450523,\n",
       "        0.00477825, 0.01237966, 0.00736477, 0.00369192, 0.00674514,\n",
       "        0.00321907, 0.01700411, 0.00306553, 0.00191609, 0.00571959,\n",
       "        0.00438711, 0.00142851, 0.00750937, 0.03262164, 0.00192926,\n",
       "        0.00926981, 0.0155889 , 0.0031741 , 0.01184781, 0.00510308,\n",
       "        0.00223901, 0.00456593, 0.00662439, 0.00146597, 0.00302979,\n",
       "        0.00647775, 0.00332906, 0.00459404, 0.00332279, 0.00286927,\n",
       "        0.00161448, 0.00529505, 0.00357211, 0.00759806, 0.00624476,\n",
       "        0.00222589, 0.00283719, 0.00489806, 0.00405005, 0.00393128,\n",
       "        0.03030177, 0.00641542, 0.00467458, 0.00753804, 0.00372551,\n",
       "        0.00604871, 0.00577805, 0.003448  , 0.00638688, 0.0075571 ,\n",
       "        0.00402124, 0.00334885, 0.00473957, 0.0051359 , 0.00361445,\n",
       "        0.00527149, 0.0032712 , 0.00395846, 0.00759161, 0.00295705,\n",
       "        0.0030354 , 0.00444762, 0.00173822, 0.00400713, 0.00889285,\n",
       "        0.00290144, 0.01481463, 0.00443128, 0.00477889, 0.00420527,\n",
       "        0.00626004, 0.0033684 , 0.01712092, 0.02009135, 0.00406573,\n",
       "        0.00386999, 0.01039428, 0.00135147, 0.00304323, 0.010305  ,\n",
       "        0.00046519, 0.01720532, 0.00463896, 0.00362532, 0.00451877,\n",
       "        0.00425631, 0.00240201, 0.0061954 , 0.02788482, 0.01186052,\n",
       "        0.00295333, 0.00640852, 0.00300108, 0.00585609, 0.00370749,\n",
       "        0.00252518, 0.00319885, 0.02682985, 0.0027731 , 0.00194251,\n",
       "        0.00464055, 0.00218963, 0.00748455, 0.00617062, 0.00399138,\n",
       "        0.00434872, 0.00817456, 0.00480075, 0.0165826 , 0.00293106,\n",
       "        0.00187939, 0.00572735, 0.00892668, 0.00439765, 0.00272391,\n",
       "        0.00463355, 0.00238201, 0.00768099, 0.00582622, 0.00537921,\n",
       "        0.00673582, 0.05385737, 0.00188727, 0.00670767, 0.00616281,\n",
       "        0.00240197, 0.00474079, 0.02155192, 0.00614435, 0.00220774,\n",
       "        0.0081511 , 0.00292225, 0.00125437, 0.00514247, 0.0046268 ,\n",
       "        0.00230891, 0.00525175, 0.00235673, 0.0088399 , 0.02344293,\n",
       "        0.00216461, 0.01323015, 0.01525114, 0.00415834, 0.00721699,\n",
       "        0.00405413, 0.0055909 , 0.00531931, 0.03863309, 0.00369328,\n",
       "        0.00859914, 0.01052574, 0.00141992, 0.00506553, 0.01311305]),\n",
       " 'mean_score_time': array([0.0076931 , 0.00879092, 0.00663509, 0.00631385, 0.00557499,\n",
       "        0.0080564 , 0.00456271, 0.00480223, 0.00687599, 0.01007643,\n",
       "        0.00608988, 0.00900841, 0.00428162, 0.00554442, 0.00706582,\n",
       "        0.00411315, 0.0099874 , 0.00747876, 0.00455985, 0.00948477,\n",
       "        0.00801396, 0.00517139, 0.00658374, 0.00774417, 0.00585623,\n",
       "        0.00720406, 0.0077652 , 0.00488129, 0.00811906, 0.00968575,\n",
       "        0.00595727, 0.00679502, 0.00815401, 0.00424261, 0.00659485,\n",
       "        0.00933428, 0.0077404 , 0.00697813, 0.00892606, 0.00636988,\n",
       "        0.00662222, 0.01100583, 0.00490627, 0.00620737, 0.00788698,\n",
       "        0.0046    , 0.00740395, 0.00919671, 0.00388751, 0.00654898,\n",
       "        0.00653672, 0.00388589, 0.0058351 , 0.00718508, 0.00519743,\n",
       "        0.00619464, 0.00798235, 0.00459146, 0.00715799, 0.00643845,\n",
       "        0.00407324, 0.00547342, 0.0069159 , 0.00674815, 0.00682039,\n",
       "        0.00890961, 0.00545034, 0.00776353, 0.00836611, 0.00491014,\n",
       "        0.00685139, 0.00883398, 0.00524764, 0.00879283, 0.01014118,\n",
       "        0.00553646, 0.00992866, 0.01053391, 0.00590138, 0.00922623,\n",
       "        0.0104619 , 0.00515819, 0.00812416, 0.01153293, 0.00641332,\n",
       "        0.00841932, 0.01056042, 0.00553455, 0.00960069, 0.01099601,\n",
       "        0.00484095, 0.0060905 , 0.00822082, 0.00421882, 0.00623589,\n",
       "        0.00773931, 0.00420251, 0.00553145, 0.00959659, 0.00430756,\n",
       "        0.00568738, 0.00957217, 0.00495501, 0.005655  , 0.00750904,\n",
       "        0.00486541, 0.00633721, 0.00797677, 0.00472236, 0.00732007,\n",
       "        0.00826669, 0.00495577, 0.00772715, 0.00995927, 0.00504351,\n",
       "        0.00816016, 0.00770922, 0.00520344, 0.00617342, 0.00886683,\n",
       "        0.00469432, 0.00796046, 0.00982528, 0.00459018, 0.00936017,\n",
       "        0.00930266, 0.00600734, 0.00789919, 0.01131539, 0.00550742,\n",
       "        0.00726666, 0.00875721, 0.00553322, 0.00656519, 0.01014776,\n",
       "        0.00400267, 0.00568724, 0.00818801, 0.00590029, 0.00627847,\n",
       "        0.00630527, 0.00480785, 0.00825138, 0.00713148, 0.00544887,\n",
       "        0.006317  , 0.01077232, 0.00445852, 0.00713134, 0.00879579,\n",
       "        0.00497222, 0.00633001, 0.00805779, 0.00589132, 0.00736246,\n",
       "        0.01060858, 0.00575714, 0.00672417, 0.00824938, 0.00436182,\n",
       "        0.00725293, 0.01015563, 0.00562501, 0.00706716, 0.00999613,\n",
       "        0.00547986, 0.00735474, 0.00928354, 0.00450521, 0.00648246,\n",
       "        0.00993543, 0.00523753, 0.00832553, 0.01070776, 0.00560598,\n",
       "        0.00838847, 0.00871558, 0.00569777, 0.00659118, 0.00872717,\n",
       "        0.00530453, 0.00740137, 0.0069756 , 0.00524621, 0.00615926,\n",
       "        0.0093514 , 0.0041831 , 0.00798521, 0.00759902, 0.0050972 ,\n",
       "        0.00649228, 0.01027546, 0.00617843, 0.00675812, 0.00773449,\n",
       "        0.00430984, 0.00589232, 0.00809317, 0.00460029, 0.00658822,\n",
       "        0.00974531, 0.00444374, 0.00725632, 0.0099308 , 0.00463724,\n",
       "        0.00786119, 0.00844679, 0.00537281, 0.00739655, 0.01063366,\n",
       "        0.00624189, 0.00701914, 0.00890708, 0.00564795, 0.00734458,\n",
       "        0.00904198, 0.00502071, 0.00957189, 0.01032977, 0.00468454,\n",
       "        0.0084352 , 0.00934772, 0.00499358, 0.00831084, 0.00871205,\n",
       "        0.00552015, 0.00629897, 0.00694408, 0.00666704, 0.00651813,\n",
       "        0.00851984, 0.00518131, 0.00657411, 0.00713644, 0.00501623,\n",
       "        0.00608521, 0.00879903, 0.00746622, 0.00696502, 0.00834455,\n",
       "        0.00445342, 0.00628934, 0.00917845, 0.00521431, 0.00694451,\n",
       "        0.00906954, 0.00451784, 0.00685902, 0.00983   , 0.00524716,\n",
       "        0.00829091, 0.00808296, 0.00511808, 0.00679436, 0.00985675,\n",
       "        0.00737004, 0.006811  , 0.00918336, 0.005862  , 0.00717793,\n",
       "        0.0085464 , 0.00586619, 0.00743051, 0.01081643, 0.00741968,\n",
       "        0.00903854, 0.01085095, 0.00511465, 0.00739059, 0.00961041,\n",
       "        0.00465899, 0.00694895, 0.01052995, 0.00680051, 0.00681086,\n",
       "        0.00807757, 0.00539279, 0.00548406, 0.00711694, 0.00573831,\n",
       "        0.00684881, 0.00760803, 0.00607033, 0.0065906 , 0.00843205,\n",
       "        0.00510449, 0.00753493, 0.00881042, 0.0058888 , 0.00677505,\n",
       "        0.01007872, 0.00517769, 0.0066762 , 0.01052127, 0.00477982,\n",
       "        0.00758681, 0.00848923, 0.00522761, 0.00732098, 0.01143503,\n",
       "        0.0047184 , 0.00912304, 0.01116743, 0.00467305, 0.00807557,\n",
       "        0.01151357, 0.00665936, 0.00828567, 0.0131484 , 0.00563622,\n",
       "        0.00772595, 0.00975623, 0.0053618 , 0.00839944, 0.00918632,\n",
       "        0.00516586, 0.00629158, 0.00904078, 0.00462718, 0.00624266,\n",
       "        0.00737944, 0.00488939, 0.00706544, 0.00873704, 0.00456047,\n",
       "        0.00667877, 0.00801525, 0.00555153, 0.00634184, 0.00754862,\n",
       "        0.00595193, 0.00572958, 0.00907693, 0.00520964, 0.00923629,\n",
       "        0.00975833, 0.00532684, 0.00894418, 0.00977068, 0.0048533 ,\n",
       "        0.00800514, 0.00901618, 0.00605454, 0.00755978, 0.01021829,\n",
       "        0.00496302, 0.00699072, 0.01245303, 0.00551682, 0.00831776,\n",
       "        0.00911093, 0.00649419, 0.00721788, 0.0113122 , 0.00707893,\n",
       "        0.00732422, 0.01149497, 0.00457215, 0.00955544, 0.00990853,\n",
       "        0.00493941, 0.00678139, 0.01083469, 0.00487704, 0.00563874,\n",
       "        0.00900021, 0.00759592, 0.00706301, 0.01002626, 0.00566063,\n",
       "        0.00600133, 0.00831056, 0.00472813, 0.00661817, 0.00966239,\n",
       "        0.00520158, 0.00635643, 0.00927982, 0.00477819, 0.00726728,\n",
       "        0.00909076, 0.00488801, 0.00847616, 0.01025915, 0.00553398,\n",
       "        0.006848  , 0.00833206, 0.00484982, 0.00772004, 0.00917263,\n",
       "        0.00468001, 0.00898151, 0.00992694, 0.00499682, 0.00828333,\n",
       "        0.01097889, 0.00545368, 0.00706406, 0.01173658, 0.00504999,\n",
       "        0.00848818, 0.01015863, 0.00469804, 0.00763035, 0.01102562,\n",
       "        0.00688047, 0.00724864, 0.00836539, 0.00571022, 0.00524144,\n",
       "        0.00863109, 0.00486636, 0.00585704, 0.00784316, 0.004953  ,\n",
       "        0.0085381 , 0.0088954 , 0.00528908, 0.0064846 , 0.00854163,\n",
       "        0.00466056, 0.00655551, 0.00882707, 0.00473418, 0.00671225,\n",
       "        0.00879316, 0.00486894, 0.00837364, 0.00849762, 0.00531507,\n",
       "        0.00664468, 0.00926328, 0.00478048, 0.00762458, 0.01073942,\n",
       "        0.00569167, 0.00768743, 0.00904703, 0.00620708, 0.00701222,\n",
       "        0.01198487, 0.00594969, 0.00749259, 0.01024203, 0.00505619,\n",
       "        0.00970373, 0.01026998, 0.00477004, 0.00695519, 0.00950975,\n",
       "        0.0043014 , 0.00635977, 0.0067513 , 0.00464892, 0.00601015,\n",
       "        0.00897861, 0.00560679, 0.00680337, 0.0074789 , 0.00601258,\n",
       "        0.00593424, 0.00722542, 0.00542836, 0.00627542, 0.00830665,\n",
       "        0.0050519 , 0.00604596, 0.00869637, 0.00537114, 0.00839162,\n",
       "        0.00915895, 0.00459666, 0.00926929, 0.01033888, 0.00472312,\n",
       "        0.00775604, 0.00836902, 0.00581961, 0.00664372, 0.01000123,\n",
       "        0.0060019 , 0.00699997, 0.00973654, 0.00591297, 0.00692267,\n",
       "        0.01018996, 0.00551376, 0.00813999, 0.0118083 , 0.00588398,\n",
       "        0.00849209, 0.00960484, 0.00712819, 0.00701885, 0.00928721,\n",
       "        0.00440836, 0.00679054, 0.01053309, 0.00491161, 0.00524673,\n",
       "        0.00825596, 0.00436258, 0.00674777, 0.00828624, 0.00739903,\n",
       "        0.00653019, 0.0075767 , 0.00441818, 0.00592208, 0.00797715,\n",
       "        0.00650487, 0.00617509, 0.00916491, 0.00433116, 0.00680032,\n",
       "        0.0087399 , 0.00597796, 0.00616975, 0.00883822, 0.00469241,\n",
       "        0.0100153 , 0.00754042, 0.00486207, 0.00815082, 0.00931702,\n",
       "        0.00499167, 0.00790753, 0.01087546, 0.00565429, 0.006286  ,\n",
       "        0.00801301, 0.00593863, 0.00935197, 0.011132  , 0.00822988,\n",
       "        0.00765972, 0.01072555, 0.00552421, 0.00649881, 0.0097548 ,\n",
       "        0.00428891, 0.00573158, 0.00808682, 0.00451465, 0.00572915,\n",
       "        0.00732532, 0.00445452, 0.00546708, 0.00741706, 0.00518136,\n",
       "        0.00563478, 0.00773382, 0.00606351, 0.00656667, 0.00823493,\n",
       "        0.00615726, 0.00657511, 0.00889373, 0.00561671, 0.00623765,\n",
       "        0.01004057, 0.00544391, 0.00664639, 0.00960388, 0.00482478,\n",
       "        0.00771151, 0.0090044 , 0.00490479, 0.00767665, 0.01114168,\n",
       "        0.00484967, 0.00694914, 0.00944681, 0.00615768, 0.00711265,\n",
       "        0.0082386 , 0.0052968 , 0.0070363 , 0.00937452, 0.00472984,\n",
       "        0.00750241, 0.00909171, 0.00555067, 0.00707502, 0.00871763,\n",
       "        0.00402865, 0.00625434, 0.00846744, 0.00528164, 0.00647326,\n",
       "        0.007693  , 0.00428567, 0.00718055, 0.01057706, 0.00446444,\n",
       "        0.00622783, 0.00848627, 0.00490055, 0.00671587, 0.00827289,\n",
       "        0.0052269 , 0.00584621, 0.01062241, 0.00466433, 0.00746403,\n",
       "        0.00866585, 0.00611105, 0.00651903, 0.00778894, 0.00477118,\n",
       "        0.00718927, 0.00859356, 0.00665283, 0.00667181, 0.01006122,\n",
       "        0.00509343, 0.00708623, 0.01020985, 0.00563121, 0.00932999,\n",
       "        0.01069546, 0.00466156, 0.00812554, 0.0113842 , 0.00476189,\n",
       "        0.00754571, 0.01091681, 0.00551362, 0.00652242, 0.01052575,\n",
       "        0.00458031, 0.0064549 , 0.00741534, 0.00448141, 0.0064127 ,\n",
       "        0.00693736, 0.00430923, 0.00622964, 0.00756917, 0.00474663,\n",
       "        0.00563755, 0.00769677, 0.0048821 , 0.00592971, 0.00891767,\n",
       "        0.00498958, 0.00765886, 0.00718203, 0.00552197, 0.006528  ,\n",
       "        0.00906997, 0.00431523, 0.00757065, 0.01202621, 0.00437584,\n",
       "        0.00678725, 0.00902758, 0.00759654, 0.00657144, 0.01061754,\n",
       "        0.00857072, 0.00939646, 0.00941901, 0.00591421, 0.00790238,\n",
       "        0.00857716, 0.00506568, 0.00738535, 0.0097888 , 0.00510807,\n",
       "        0.00818028, 0.00968952, 0.00641127, 0.00632071, 0.0056386 ]),\n",
       " 'std_score_time': array([2.35312645e-03, 2.84815384e-03, 1.77058110e-03, 3.45365659e-03,\n",
       "        9.29360018e-04, 2.58952934e-03, 1.22425229e-03, 3.22480214e-04,\n",
       "        7.45310161e-04, 3.25316658e-03, 1.23664963e-03, 4.50744295e-03,\n",
       "        6.48193298e-04, 8.26884937e-04, 7.80556611e-04, 5.03055324e-04,\n",
       "        8.09761741e-03, 1.10451629e-03, 1.09428717e-03, 2.86968131e-03,\n",
       "        5.08276471e-04, 8.28715909e-04, 1.03506094e-03, 1.54980909e-03,\n",
       "        1.88552615e-03, 2.15418575e-03, 7.34334357e-04, 9.65891282e-04,\n",
       "        2.23855871e-03, 2.14373019e-03, 2.06109575e-03, 1.55955238e-03,\n",
       "        6.04525502e-04, 2.54777418e-04, 9.84361169e-04, 1.58616236e-03,\n",
       "        5.64146488e-03, 8.42123428e-04, 6.86541169e-04, 1.54605595e-03,\n",
       "        6.12008494e-04, 4.00131259e-03, 7.71385745e-04, 6.83763280e-04,\n",
       "        3.25835416e-04, 1.63795664e-03, 1.63520336e-03, 3.15254768e-03,\n",
       "        1.22924476e-04, 1.67841908e-03, 9.61565486e-04, 3.07948459e-04,\n",
       "        1.43303051e-03, 1.14273788e-03, 1.65103519e-03, 9.71669921e-04,\n",
       "        1.67492020e-03, 6.92025506e-04, 1.13339604e-03, 7.68458645e-04,\n",
       "        1.91841527e-04, 3.46704340e-04, 2.90456563e-04, 2.45437782e-03,\n",
       "        8.80821227e-04, 1.23612775e-03, 1.32719856e-03, 1.48113851e-03,\n",
       "        8.07466518e-04, 1.03614935e-03, 1.06769217e-03, 1.14274756e-03,\n",
       "        9.98265502e-04, 3.33811041e-03, 1.46979888e-03, 1.08931164e-03,\n",
       "        2.41680137e-03, 2.11224358e-03, 1.35529031e-03, 1.78303972e-03,\n",
       "        1.33675506e-03, 4.19794365e-04, 2.02128657e-03, 1.78898872e-03,\n",
       "        2.02592715e-03, 1.28433623e-03, 2.14732004e-03, 9.27517560e-04,\n",
       "        3.46449959e-03, 3.26736589e-03, 9.87943091e-04, 1.50959343e-03,\n",
       "        2.09822994e-03, 2.70008467e-04, 1.78998390e-03, 1.30447355e-03,\n",
       "        2.44588069e-04, 5.69198952e-04, 2.01667182e-03, 4.02599952e-04,\n",
       "        2.56009052e-04, 2.32073444e-03, 1.53753452e-03, 2.87154306e-04,\n",
       "        9.90657977e-04, 7.83852008e-04, 6.95993506e-04, 1.24571719e-03,\n",
       "        7.50676834e-04, 1.31217841e-03, 6.40477528e-04, 7.71197435e-04,\n",
       "        1.66913084e-03, 3.92514147e-03, 9.67265874e-04, 1.49645190e-03,\n",
       "        3.43373096e-04, 7.53213331e-04, 2.64999195e-04, 9.17568507e-04,\n",
       "        5.78228330e-04, 1.63796656e-03, 1.72454170e-03, 4.65014046e-04,\n",
       "        2.81575021e-03, 1.20259645e-03, 1.49485068e-03, 1.30828654e-03,\n",
       "        3.34822958e-03, 1.36307714e-03, 7.85147627e-04, 4.08941774e-04,\n",
       "        1.03643743e-03, 4.77540942e-04, 2.34417185e-03, 1.54243562e-04,\n",
       "        1.15471112e-03, 1.64410724e-03, 2.00534708e-03, 8.62579907e-04,\n",
       "        3.81864594e-04, 8.64056780e-04, 4.70091247e-03, 1.61940218e-03,\n",
       "        1.63479599e-03, 8.17813352e-04, 4.92818495e-03, 2.37915395e-04,\n",
       "        1.42961594e-03, 1.13943953e-03, 1.36657279e-03, 1.08490746e-03,\n",
       "        1.54035358e-03, 1.82051541e-03, 1.33010784e-03, 2.29413691e-03,\n",
       "        1.79108592e-03, 6.77122321e-04, 3.69786358e-04, 1.23926519e-04,\n",
       "        2.11899098e-03, 2.47422280e-03, 1.06995623e-03, 9.02307609e-04,\n",
       "        6.71664693e-04, 9.35152356e-04, 1.51972144e-03, 8.98687694e-04,\n",
       "        9.94812371e-05, 5.75626821e-04, 2.47440157e-03, 1.11939295e-03,\n",
       "        1.40651193e-03, 1.48686123e-03, 1.22192350e-03, 8.37612828e-04,\n",
       "        2.32891847e-04, 1.87146984e-03, 1.34283778e-04, 9.55735299e-04,\n",
       "        1.14814764e-03, 2.03337198e-03, 5.64218933e-04, 9.62469533e-04,\n",
       "        1.14278698e-03, 1.50534354e-03, 4.00064654e-04, 2.59691675e-03,\n",
       "        1.56508763e-03, 9.08477812e-04, 1.47541913e-03, 2.45850209e-03,\n",
       "        2.32751694e-03, 2.08142882e-03, 8.15426764e-04, 2.00373499e-04,\n",
       "        3.11083877e-04, 7.98657536e-04, 3.31234959e-04, 7.91651435e-04,\n",
       "        1.52179441e-03, 2.13433910e-04, 1.11965367e-03, 1.76197023e-03,\n",
       "        4.39034617e-04, 1.95074259e-03, 8.90086592e-04, 1.33639909e-03,\n",
       "        7.65220935e-04, 1.78946126e-03, 7.22238417e-04, 5.78884034e-04,\n",
       "        1.19219968e-03, 1.55936818e-03, 9.55200555e-04, 6.95290066e-04,\n",
       "        7.01818733e-04, 8.51503317e-04, 1.88206560e-03, 5.12089644e-04,\n",
       "        1.66035392e-03, 4.84842348e-04, 6.59086945e-04, 1.22772191e-03,\n",
       "        6.22465699e-04, 1.29446248e-03, 8.22510392e-04, 6.18341618e-04,\n",
       "        2.66788979e-03, 1.65009212e-03, 2.79182593e-03, 1.05991955e-03,\n",
       "        1.85425448e-03, 1.06276742e-03, 1.53926673e-03, 5.22189146e-04,\n",
       "        1.90254287e-03, 2.31631604e-03, 1.38269388e-03, 7.45369748e-04,\n",
       "        4.48683673e-04, 6.51772917e-04, 2.16877965e-03, 1.10159254e-03,\n",
       "        6.44909919e-04, 1.39610689e-03, 2.60806187e-04, 1.15918321e-03,\n",
       "        1.44881485e-03, 9.39158685e-04, 3.35083628e-03, 4.77793583e-04,\n",
       "        8.74359778e-04, 3.96156546e-04, 8.30560164e-04, 1.92326963e-03,\n",
       "        5.37536851e-04, 9.04925679e-04, 1.31501654e-03, 1.34074113e-03,\n",
       "        4.35199596e-04, 1.64202204e-03, 7.03852114e-04, 7.45712010e-04,\n",
       "        3.10543926e-03, 1.91149280e-03, 4.05230048e-03, 3.59759293e-04,\n",
       "        1.13110418e-03, 1.91515369e-03, 3.85292048e-04, 1.27557494e-03,\n",
       "        4.22975105e-03, 8.65511217e-04, 2.00623015e-03, 1.32070947e-03,\n",
       "        1.31949671e-03, 4.05877039e-04, 4.30013499e-04, 1.57377865e-03,\n",
       "        1.78107028e-03, 6.06838367e-04, 2.33062083e-03, 1.36732967e-03,\n",
       "        9.93205130e-04, 9.14741236e-04, 1.66226616e-03, 2.58015195e-03,\n",
       "        1.45190978e-03, 5.70109398e-04, 1.72895685e-03, 8.55363307e-04,\n",
       "        6.56928258e-04, 2.49672528e-03, 3.24658423e-04, 1.59164953e-03,\n",
       "        7.85306496e-04, 1.49852210e-03, 5.42010828e-04, 1.24692269e-03,\n",
       "        3.28173889e-04, 1.95439578e-03, 2.73821979e-03, 1.89015591e-04,\n",
       "        7.66708683e-04, 1.65662285e-03, 9.55952137e-04, 1.17778420e-03,\n",
       "        2.04087657e-03, 6.89170288e-04, 1.19610003e-03, 7.45109543e-04,\n",
       "        9.32226461e-04, 2.04182038e-03, 6.03779871e-04, 9.30450618e-04,\n",
       "        1.02924849e-03, 3.41046701e-03, 5.42744935e-04, 1.10374705e-03,\n",
       "        1.27867608e-03, 1.21778081e-03, 1.29894861e-03, 3.09977339e-03,\n",
       "        1.99200571e-04, 1.21727187e-03, 6.39389905e-04, 1.69310050e-03,\n",
       "        4.42518849e-04, 4.21887329e-04, 1.59036066e-03, 1.87180214e-04,\n",
       "        1.41823350e-03, 1.27605187e-03, 2.72710998e-03, 1.61476847e-03,\n",
       "        4.82989452e-04, 2.12336860e-03, 1.26469634e-03, 5.79780208e-04,\n",
       "        1.09487285e-03, 8.03232591e-04, 1.28292729e-03, 5.18028370e-04,\n",
       "        1.62897301e-03, 4.98542897e-04, 5.68398401e-04, 1.95455808e-03,\n",
       "        9.05212224e-04, 1.21561936e-03, 1.46347772e-03, 1.48423406e-03,\n",
       "        3.30719245e-04, 1.62377897e-03, 2.51996063e-03, 7.41263400e-04,\n",
       "        1.06210122e-03, 2.92156886e-04, 2.97767630e-03, 1.81862803e-03,\n",
       "        5.73179876e-04, 1.86404637e-03, 2.92189460e-03, 6.36676098e-04,\n",
       "        4.59538353e-04, 2.71877617e-03, 3.08589158e-03, 1.39308053e-03,\n",
       "        3.10064935e-03, 1.53273395e-03, 3.39773716e-04, 9.88840739e-04,\n",
       "        4.54278193e-04, 1.46892569e-03, 2.34912908e-03, 1.24852377e-03,\n",
       "        9.04919988e-04, 2.29943098e-03, 4.67046148e-04, 6.36434796e-04,\n",
       "        1.59320291e-03, 7.33901670e-04, 8.63862214e-04, 1.67198257e-03,\n",
       "        8.30196162e-04, 9.68059569e-04, 7.33040639e-04, 2.23408951e-04,\n",
       "        1.64320910e-03, 5.55266570e-04, 2.10324574e-04, 4.06268852e-03,\n",
       "        1.23369472e-03, 4.67944417e-04, 1.82668626e-03, 1.87456474e-03,\n",
       "        7.16839128e-04, 3.31445334e-04, 8.86169016e-04, 7.90903713e-04,\n",
       "        1.95927102e-03, 1.51638574e-03, 3.35516632e-04, 1.52520683e-03,\n",
       "        2.13694364e-03, 2.43430280e-03, 2.49680173e-03, 2.06328505e-03,\n",
       "        1.81988291e-03, 3.36235371e-04, 2.10897531e-03, 1.01706856e-03,\n",
       "        1.30631183e-03, 1.72974365e-03, 5.73451722e-04, 3.00153470e-03,\n",
       "        1.79977804e-03, 1.12512082e-03, 2.36220171e-04, 1.37511195e-03,\n",
       "        3.16578772e-04, 1.05548649e-03, 1.68810660e-03, 1.93645537e-04,\n",
       "        8.97968595e-04, 7.37883746e-04, 6.46882347e-04, 1.53485691e-03,\n",
       "        6.05769215e-04, 1.59603139e-03, 6.64399774e-04, 1.34590064e-03,\n",
       "        2.55699826e-04, 8.92652933e-04, 1.27016456e-03, 1.20299858e-03,\n",
       "        1.14983300e-03, 6.59272734e-04, 3.03298072e-03, 5.88005813e-04,\n",
       "        2.92929316e-03, 1.25026276e-03, 6.45863801e-04, 1.00710432e-03,\n",
       "        6.89946284e-04, 2.24424803e-03, 1.44265320e-03, 7.40984185e-04,\n",
       "        5.56950116e-04, 9.16507519e-04, 3.43376943e-04, 1.48888540e-03,\n",
       "        1.28443706e-03, 4.73493728e-04, 8.77412150e-04, 1.73708162e-03,\n",
       "        1.43812536e-03, 1.41689443e-03, 1.18324101e-03, 1.96201184e-03,\n",
       "        4.09042966e-04, 3.47454868e-04, 1.13867797e-03, 9.70624190e-04,\n",
       "        9.67654700e-04, 1.17779449e-03, 4.84415267e-04, 2.22745037e-03,\n",
       "        1.58707674e-03, 1.66105765e-03, 9.84480953e-04, 5.30123620e-04,\n",
       "        2.36324261e-03, 2.06097872e-03, 5.02833237e-04, 1.33077412e-03,\n",
       "        1.19597606e-03, 1.15113358e-03, 4.13406057e-04, 2.20012120e-03,\n",
       "        2.71638551e-03, 7.49813582e-04, 2.07989141e-03, 1.55039609e-03,\n",
       "        4.36156497e-04, 1.20646525e-03, 1.75885614e-03, 1.35290836e-03,\n",
       "        2.73160156e-03, 1.63478785e-03, 2.00561084e-03, 1.29979187e-03,\n",
       "        1.99414850e-03, 1.01081091e-03, 2.01215140e-03, 6.94064142e-04,\n",
       "        1.81815384e-03, 2.91132252e-03, 5.97631833e-04, 1.93588170e-04,\n",
       "        1.90929569e-03, 3.94182478e-04, 2.09862386e-03, 1.06361588e-03,\n",
       "        2.53928355e-03, 1.18304591e-03, 9.59650318e-04, 4.75437384e-04,\n",
       "        5.56691913e-04, 8.35583450e-04, 1.89422576e-03, 1.03509083e-03,\n",
       "        1.58366833e-03, 1.86106825e-04, 1.39701467e-03, 7.51477834e-04,\n",
       "        1.44819590e-03, 3.45423069e-04, 1.52721193e-03, 3.47710087e-04,\n",
       "        2.35414434e-03, 4.00372662e-04, 4.58999669e-04, 1.24507342e-03,\n",
       "        7.03740918e-04, 4.77052130e-04, 2.34345358e-03, 5.30364246e-03,\n",
       "        1.73836516e-03, 3.46252130e-04, 7.52660576e-04, 1.20207155e-03,\n",
       "        2.19547637e-03, 2.63905831e-03, 3.74926145e-03, 2.37282393e-03,\n",
       "        2.86327012e-03, 1.09197058e-03, 2.96220937e-04, 1.49061484e-03,\n",
       "        3.61312293e-04, 1.37611576e-03, 1.08020165e-03, 5.69752374e-04,\n",
       "        4.87321450e-04, 5.06992085e-04, 4.53405536e-04, 3.83968887e-04,\n",
       "        1.66659321e-03, 1.13561984e-03, 8.37968556e-05, 1.07510351e-03,\n",
       "        2.06148593e-03, 1.22148599e-03, 1.37026748e-03, 2.32181013e-03,\n",
       "        1.07011790e-03, 2.46209989e-03, 2.11260865e-03, 4.41458524e-04,\n",
       "        1.81580948e-03, 1.75020622e-03, 8.18620987e-04, 2.02235216e-03,\n",
       "        1.08573831e-03, 1.38562469e-03, 1.61259199e-03, 4.82404405e-04,\n",
       "        1.22660079e-03, 9.04712084e-04, 6.96447433e-04, 8.72362542e-04,\n",
       "        2.17622304e-04, 2.00491081e-03, 1.54642853e-03, 6.47796668e-04,\n",
       "        1.27347850e-03, 5.67636867e-04, 9.07455923e-04, 5.57503654e-04,\n",
       "        1.08891888e-03, 9.13992810e-04, 1.97538651e-03, 1.01632861e-03,\n",
       "        4.54906588e-04, 1.59148614e-04, 1.65407841e-03, 1.66613689e-03,\n",
       "        1.29684894e-03, 1.88129460e-03, 1.52034445e-03, 3.95589164e-04,\n",
       "        2.41556046e-03, 4.77065286e-03, 5.07878257e-04, 3.47344224e-04,\n",
       "        1.75484812e-03, 1.30298635e-03, 1.34612360e-03, 2.10585133e-03,\n",
       "        1.05246669e-03, 5.34760137e-04, 2.44965069e-03, 4.10963858e-04,\n",
       "        1.35531801e-03, 5.66984754e-04, 2.22513830e-03, 5.73423744e-04,\n",
       "        7.26110637e-04, 4.97731328e-04, 2.08613609e-03, 7.48198422e-04,\n",
       "        2.51394543e-03, 4.82883048e-04, 1.21255506e-03, 1.36912180e-03,\n",
       "        8.24065410e-04, 1.29074318e-03, 1.66866345e-03, 1.89868984e-03,\n",
       "        1.68667227e-03, 3.99075299e-04, 1.22173920e-03, 2.36776031e-03,\n",
       "        4.15534006e-04, 1.10331405e-03, 2.57258948e-03, 1.17634705e-03,\n",
       "        6.10183846e-04, 1.96312893e-03, 8.77460670e-04, 1.18576896e-03,\n",
       "        8.69222627e-04, 3.74233868e-04, 1.71409560e-03, 6.80559502e-04,\n",
       "        2.81670393e-04, 1.06111447e-03, 2.22402366e-03, 8.45329867e-04,\n",
       "        1.37555545e-04, 1.61417677e-03, 6.89544436e-04, 7.03878887e-04,\n",
       "        1.18952125e-03, 1.31765823e-03, 2.25955022e-03, 2.74126943e-04,\n",
       "        1.71685659e-03, 7.23812130e-04, 1.07196627e-03, 2.25297993e-04,\n",
       "        1.17800068e-03, 3.31710126e-03, 3.55205297e-04, 1.26246811e-03,\n",
       "        2.33935525e-03, 3.16851007e-03, 4.63911771e-04, 1.70543533e-03,\n",
       "        6.01429745e-03, 6.17833774e-03, 3.02474028e-04, 1.94929906e-03,\n",
       "        2.31209905e-03, 7.03202856e-04, 5.61813781e-04, 9.19935072e-04,\n",
       "        1.84382974e-03, 1.06842089e-03, 1.57694126e-03, 5.30792250e-04,\n",
       "        2.19385182e-03, 3.33152920e-04, 3.89701480e-04]),\n",
       " 'param_colsample_bytree': masked_array(data=[0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_learning_rate': masked_array(data=[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_max_depth': masked_array(data=[3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_min_child_weight': masked_array(data=[1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_n_estimators': masked_array(data=[100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500}],\n",
       " 'split0_test_score': array([0.83168317, 0.85436893, 0.8627451 , 0.85148515, 0.87378641,\n",
       "        0.85714286, 0.8627451 , 0.85714286, 0.86538462, 0.83168317,\n",
       "        0.84615385, 0.80769231, 0.85148515, 0.83018868, 0.83809524,\n",
       "        0.84615385, 0.85714286, 0.84313725, 0.8627451 , 0.84313725,\n",
       "        0.83809524, 0.87128713, 0.85436893, 0.83495146, 0.88235294,\n",
       "        0.8627451 , 0.83495146, 0.84615385, 0.83495146, 0.82242991,\n",
       "        0.8627451 , 0.8627451 , 0.83495146, 0.86538462, 0.85436893,\n",
       "        0.84313725, 0.85436893, 0.81904762, 0.8       , 0.85148515,\n",
       "        0.85436893, 0.83809524, 0.87378641, 0.85148515, 0.83495146,\n",
       "        0.82828283, 0.85436893, 0.83809524, 0.84848485, 0.87378641,\n",
       "        0.86538462, 0.8627451 , 0.86538462, 0.86538462, 0.83168317,\n",
       "        0.85436893, 0.84615385, 0.84313725, 0.8627451 , 0.83809524,\n",
       "        0.8627451 , 0.86538462, 0.85436893, 0.86      , 0.8627451 ,\n",
       "        0.86538462, 0.85148515, 0.84313725, 0.85436893, 0.8627451 ,\n",
       "        0.87378641, 0.87128713, 0.8627451 , 0.85148515, 0.83495146,\n",
       "        0.8627451 , 0.85436893, 0.85436893, 0.8627451 , 0.85436893,\n",
       "        0.84313725, 0.8627451 , 0.84313725, 0.81904762, 0.85436893,\n",
       "        0.85436893, 0.85436893, 0.85436893, 0.85436893, 0.85148515,\n",
       "        0.82      , 0.85148515, 0.85436893, 0.83673469, 0.8627451 ,\n",
       "        0.86538462, 0.84848485, 0.85436893, 0.86538462, 0.81188119,\n",
       "        0.86538462, 0.86538462, 0.81188119, 0.85714286, 0.85714286,\n",
       "        0.81188119, 0.87378641, 0.85714286, 0.84848485, 0.8627451 ,\n",
       "        0.8627451 , 0.84      , 0.85436893, 0.81553398, 0.83495146,\n",
       "        0.86538462, 0.8490566 , 0.84      , 0.8627451 , 0.85148515,\n",
       "        0.84313725, 0.86538462, 0.85436893, 0.83495146, 0.85714286,\n",
       "        0.85714286, 0.85148515, 0.85148515, 0.84615385, 0.85436893,\n",
       "        0.84615385, 0.85436893, 0.83495146, 0.86792453, 0.85148515,\n",
       "        0.82828283, 0.84      , 0.83495146, 0.84848485, 0.85148515,\n",
       "        0.8627451 , 0.83673469, 0.85436893, 0.84615385, 0.82474227,\n",
       "        0.83168317, 0.8627451 , 0.82828283, 0.84313725, 0.8627451 ,\n",
       "        0.84      , 0.84313725, 0.87378641, 0.83673469, 0.86      ,\n",
       "        0.85436893, 0.83673469, 0.86      , 0.8627451 , 0.84      ,\n",
       "        0.8627451 , 0.86538462, 0.82828283, 0.8627451 , 0.8627451 ,\n",
       "        0.84      , 0.87378641, 0.87378641, 0.82828283, 0.85148515,\n",
       "        0.86538462, 0.83168317, 0.85148515, 0.85148515, 0.84      ,\n",
       "        0.87378641, 0.8627451 , 0.84      , 0.85436893, 0.85436893,\n",
       "        0.82105263, 0.83673469, 0.84848485, 0.82105263, 0.83673469,\n",
       "        0.84848485, 0.80851064, 0.83673469, 0.82828283, 0.8       ,\n",
       "        0.83673469, 0.84848485, 0.82105263, 0.83673469, 0.84848485,\n",
       "        0.8       , 0.83673469, 0.84      , 0.80851064, 0.83673469,\n",
       "        0.84848485, 0.82105263, 0.84848485, 0.84848485, 0.78723404,\n",
       "        0.84313725, 0.84      , 0.8       , 0.83673469, 0.83673469,\n",
       "        0.85416667, 0.82828283, 0.84      , 0.83673469, 0.84313725,\n",
       "        0.84313725, 0.83333333, 0.82828283, 0.82828283, 0.84536082,\n",
       "        0.82828283, 0.84      , 0.82828283, 0.84313725, 0.84313725,\n",
       "        0.83168317, 0.85436893, 0.8627451 , 0.85148515, 0.87378641,\n",
       "        0.85714286, 0.8627451 , 0.85714286, 0.86538462, 0.83168317,\n",
       "        0.84615385, 0.80769231, 0.85148515, 0.83018868, 0.83809524,\n",
       "        0.84615385, 0.85714286, 0.84313725, 0.8627451 , 0.84313725,\n",
       "        0.83809524, 0.87128713, 0.85436893, 0.83495146, 0.88235294,\n",
       "        0.8627451 , 0.83495146, 0.84615385, 0.83495146, 0.82242991,\n",
       "        0.8627451 , 0.8627451 , 0.83495146, 0.86538462, 0.85436893,\n",
       "        0.84313725, 0.85436893, 0.81904762, 0.8       , 0.85148515,\n",
       "        0.85436893, 0.83809524, 0.87378641, 0.85148515, 0.83495146,\n",
       "        0.82828283, 0.85436893, 0.83809524, 0.84848485, 0.87378641,\n",
       "        0.86538462, 0.8627451 , 0.86538462, 0.86538462, 0.83168317,\n",
       "        0.85436893, 0.84615385, 0.84313725, 0.8627451 , 0.83809524,\n",
       "        0.8627451 , 0.86538462, 0.85436893, 0.86      , 0.8627451 ,\n",
       "        0.86538462, 0.85148515, 0.84313725, 0.85436893, 0.8627451 ,\n",
       "        0.87378641, 0.87128713, 0.8627451 , 0.85148515, 0.83495146,\n",
       "        0.8627451 , 0.85436893, 0.85436893, 0.8627451 , 0.85436893,\n",
       "        0.84313725, 0.8627451 , 0.84313725, 0.81904762, 0.85436893,\n",
       "        0.85436893, 0.85436893, 0.85436893, 0.85436893, 0.85148515,\n",
       "        0.82      , 0.85148515, 0.85436893, 0.83673469, 0.8627451 ,\n",
       "        0.86538462, 0.84848485, 0.85436893, 0.86538462, 0.81188119,\n",
       "        0.86538462, 0.86538462, 0.81188119, 0.85714286, 0.85714286,\n",
       "        0.81188119, 0.87378641, 0.85714286, 0.84848485, 0.8627451 ,\n",
       "        0.8627451 , 0.84      , 0.85436893, 0.81553398, 0.83495146,\n",
       "        0.86538462, 0.8490566 , 0.84      , 0.8627451 , 0.85148515,\n",
       "        0.84313725, 0.86538462, 0.85436893, 0.83495146, 0.85714286,\n",
       "        0.85714286, 0.85148515, 0.85148515, 0.84615385, 0.85436893,\n",
       "        0.84615385, 0.85436893, 0.83495146, 0.86792453, 0.85148515,\n",
       "        0.82828283, 0.84      , 0.83495146, 0.84848485, 0.85148515,\n",
       "        0.8627451 , 0.83673469, 0.85436893, 0.84615385, 0.82474227,\n",
       "        0.83168317, 0.8627451 , 0.82828283, 0.84313725, 0.8627451 ,\n",
       "        0.84      , 0.84313725, 0.87378641, 0.83673469, 0.86      ,\n",
       "        0.85436893, 0.83673469, 0.86      , 0.8627451 , 0.84      ,\n",
       "        0.8627451 , 0.86538462, 0.82828283, 0.8627451 , 0.8627451 ,\n",
       "        0.84      , 0.87378641, 0.87378641, 0.82828283, 0.85148515,\n",
       "        0.86538462, 0.83168317, 0.85148515, 0.85148515, 0.84      ,\n",
       "        0.87378641, 0.8627451 , 0.84      , 0.85436893, 0.85436893,\n",
       "        0.82105263, 0.83673469, 0.84848485, 0.82105263, 0.83673469,\n",
       "        0.84848485, 0.80851064, 0.83673469, 0.82828283, 0.8       ,\n",
       "        0.83673469, 0.84848485, 0.82105263, 0.83673469, 0.84848485,\n",
       "        0.8       , 0.83673469, 0.84      , 0.80851064, 0.83673469,\n",
       "        0.84848485, 0.82105263, 0.84848485, 0.84848485, 0.78723404,\n",
       "        0.84313725, 0.84      , 0.8       , 0.83673469, 0.83673469,\n",
       "        0.85416667, 0.82828283, 0.84      , 0.83673469, 0.84313725,\n",
       "        0.84313725, 0.83333333, 0.82828283, 0.82828283, 0.84536082,\n",
       "        0.82828283, 0.84      , 0.82828283, 0.84313725, 0.84313725,\n",
       "        0.82352941, 0.85148515, 0.84      , 0.86      , 0.87378641,\n",
       "        0.85714286, 0.85436893, 0.85714286, 0.85714286, 0.81553398,\n",
       "        0.83018868, 0.81904762, 0.84615385, 0.85714286, 0.85714286,\n",
       "        0.83495146, 0.86538462, 0.84615385, 0.83495146, 0.81553398,\n",
       "        0.80392157, 0.85436893, 0.84313725, 0.81904762, 0.85436893,\n",
       "        0.85436893, 0.83495146, 0.82692308, 0.83809524, 0.7961165 ,\n",
       "        0.8627451 , 0.85436893, 0.85436893, 0.84      , 0.86538462,\n",
       "        0.84615385, 0.85148515, 0.83168317, 0.80769231, 0.8627451 ,\n",
       "        0.8627451 , 0.87128713, 0.85436893, 0.8411215 , 0.82692308,\n",
       "        0.83168317, 0.83495146, 0.83809524, 0.84      , 0.8627451 ,\n",
       "        0.86538462, 0.84313725, 0.87378641, 0.86538462, 0.82352941,\n",
       "        0.85436893, 0.83809524, 0.83168317, 0.84615385, 0.85436893,\n",
       "        0.82352941, 0.85436893, 0.85436893, 0.82352941, 0.83495146,\n",
       "        0.83495146, 0.83168317, 0.84313725, 0.84313725, 0.82352941,\n",
       "        0.84615385, 0.86538462, 0.84615385, 0.83495146, 0.82692308,\n",
       "        0.85436893, 0.84313725, 0.84313725, 0.83495146, 0.85714286,\n",
       "        0.83809524, 0.84      , 0.84313725, 0.81553398, 0.8627451 ,\n",
       "        0.85148515, 0.8627451 , 0.85436893, 0.85714286, 0.85436893,\n",
       "        0.80392157, 0.82352941, 0.83809524, 0.85148515, 0.85148515,\n",
       "        0.8627451 , 0.84313725, 0.84313725, 0.86538462, 0.82352941,\n",
       "        0.83495146, 0.85436893, 0.84      , 0.85436893, 0.84615385,\n",
       "        0.82352941, 0.84313725, 0.84615385, 0.80769231, 0.84313725,\n",
       "        0.85148515, 0.82352941, 0.85436893, 0.84615385, 0.81553398,\n",
       "        0.83495146, 0.83809524, 0.80392157, 0.84313725, 0.84313725,\n",
       "        0.83168317, 0.84313725, 0.83495146, 0.81553398, 0.85436893,\n",
       "        0.85714286, 0.83168317, 0.84313725, 0.84313725, 0.8627451 ,\n",
       "        0.85148515, 0.84      , 0.82352941, 0.86538462, 0.84615385,\n",
       "        0.82      , 0.82352941, 0.83495146, 0.84536082, 0.84313725,\n",
       "        0.85148515, 0.81632653, 0.84615385, 0.85436893, 0.8       ,\n",
       "        0.83168317, 0.85436893, 0.80808081, 0.84313725, 0.85436893,\n",
       "        0.76767677, 0.85148515, 0.85148515, 0.8       , 0.83168317,\n",
       "        0.8627451 , 0.81632653, 0.84313725, 0.8627451 , 0.7755102 ,\n",
       "        0.82352941, 0.8627451 , 0.82828283, 0.83168317, 0.84313725,\n",
       "        0.82474227, 0.85148515, 0.85436893, 0.79591837, 0.84      ,\n",
       "        0.8627451 , 0.81632653, 0.85148515, 0.85148515, 0.82474227,\n",
       "        0.85148515, 0.8627451 , 0.79591837, 0.84313725, 0.86538462,\n",
       "        0.75555556, 0.8125    , 0.81188119, 0.76404494, 0.82105263,\n",
       "        0.84      , 0.74157303, 0.81632653, 0.84615385, 0.76595745,\n",
       "        0.79591837, 0.84      , 0.77777778, 0.80412371, 0.84848485,\n",
       "        0.71111111, 0.7628866 , 0.82352941, 0.76595745, 0.82      ,\n",
       "        0.83168317, 0.80434783, 0.81632653, 0.84      , 0.75268817,\n",
       "        0.79591837, 0.83168317, 0.77894737, 0.80808081, 0.82      ,\n",
       "        0.78723404, 0.81632653, 0.84      , 0.76086957, 0.78350515,\n",
       "        0.8       , 0.75      , 0.82828283, 0.82      , 0.7826087 ,\n",
       "        0.82474227, 0.82828283, 0.76923077, 0.79591837, 0.83168317]),\n",
       " 'split1_test_score': array([0.73469388, 0.72164948, 0.7       , 0.72164948, 0.70833333,\n",
       "        0.72727273, 0.72164948, 0.71428571, 0.73469388, 0.70212766,\n",
       "        0.69387755, 0.70588235, 0.72340426, 0.72727273, 0.72      ,\n",
       "        0.71578947, 0.72916667, 0.75      , 0.69473684, 0.71428571,\n",
       "        0.67307692, 0.69473684, 0.69387755, 0.71287129, 0.72916667,\n",
       "        0.73469388, 0.76      , 0.70833333, 0.71287129, 0.69230769,\n",
       "        0.67368421, 0.74      , 0.7254902 , 0.72164948, 0.74226804,\n",
       "        0.76767677, 0.72164948, 0.70588235, 0.69902913, 0.68817204,\n",
       "        0.73267327, 0.7184466 , 0.70833333, 0.76      , 0.76767677,\n",
       "        0.72164948, 0.70103093, 0.71428571, 0.76767677, 0.72164948,\n",
       "        0.71428571, 0.74226804, 0.72916667, 0.72164948, 0.70212766,\n",
       "        0.70833333, 0.69387755, 0.71578947, 0.72727273, 0.69387755,\n",
       "        0.70833333, 0.70833333, 0.73469388, 0.66666667, 0.70833333,\n",
       "        0.72      , 0.65957447, 0.70833333, 0.72      , 0.71578947,\n",
       "        0.73469388, 0.75510204, 0.66666667, 0.72727273, 0.70588235,\n",
       "        0.69473684, 0.70103093, 0.74      , 0.71578947, 0.73469388,\n",
       "        0.75510204, 0.66666667, 0.72727273, 0.7184466 , 0.69387755,\n",
       "        0.72727273, 0.75247525, 0.71578947, 0.74226804, 0.7755102 ,\n",
       "        0.69387755, 0.71428571, 0.72727273, 0.72164948, 0.70833333,\n",
       "        0.74747475, 0.72164948, 0.73469388, 0.71428571, 0.69473684,\n",
       "        0.72916667, 0.72164948, 0.70833333, 0.70103093, 0.72164948,\n",
       "        0.70833333, 0.70833333, 0.70833333, 0.68041237, 0.70833333,\n",
       "        0.73267327, 0.67368421, 0.6875    , 0.69387755, 0.71578947,\n",
       "        0.70833333, 0.70833333, 0.69473684, 0.72164948, 0.72      ,\n",
       "        0.67368421, 0.68085106, 0.72      , 0.69473684, 0.70103093,\n",
       "        0.74747475, 0.68085106, 0.70833333, 0.72      , 0.68085106,\n",
       "        0.69473684, 0.72727273, 0.69473684, 0.70103093, 0.74747475,\n",
       "        0.70833333, 0.72164948, 0.72164948, 0.68817204, 0.72727273,\n",
       "        0.70833333, 0.70833333, 0.72164948, 0.70833333, 0.68085106,\n",
       "        0.69473684, 0.70833333, 0.68085106, 0.70103093, 0.71578947,\n",
       "        0.70212766, 0.70103093, 0.70833333, 0.69473684, 0.6875    ,\n",
       "        0.70833333, 0.6875    , 0.6875    , 0.71578947, 0.70103093,\n",
       "        0.70833333, 0.6875    , 0.68085106, 0.69473684, 0.69473684,\n",
       "        0.67368421, 0.67368421, 0.70833333, 0.70833333, 0.70103093,\n",
       "        0.6875    , 0.69473684, 0.69473684, 0.74747475, 0.66666667,\n",
       "        0.67368421, 0.6875    , 0.70103093, 0.70103093, 0.69473684,\n",
       "        0.67391304, 0.66666667, 0.70103093, 0.66666667, 0.65957447,\n",
       "        0.71428571, 0.66666667, 0.68041237, 0.6875    , 0.65957447,\n",
       "        0.67368421, 0.6875    , 0.65263158, 0.65957447, 0.70103093,\n",
       "        0.66666667, 0.65979381, 0.70103093, 0.66666667, 0.68041237,\n",
       "        0.69387755, 0.66666667, 0.6875    , 0.71428571, 0.6875    ,\n",
       "        0.67346939, 0.70103093, 0.69473684, 0.68686869, 0.69387755,\n",
       "        0.66666667, 0.66666667, 0.6875    , 0.6875    , 0.68041237,\n",
       "        0.6875    , 0.67368421, 0.68041237, 0.68041237, 0.65979381,\n",
       "        0.67346939, 0.6875    , 0.70103093, 0.67346939, 0.69387755,\n",
       "        0.73469388, 0.72164948, 0.7       , 0.72164948, 0.70833333,\n",
       "        0.72727273, 0.72164948, 0.71428571, 0.73469388, 0.70212766,\n",
       "        0.69387755, 0.70588235, 0.72340426, 0.72727273, 0.72      ,\n",
       "        0.71578947, 0.72916667, 0.75      , 0.69473684, 0.71428571,\n",
       "        0.67307692, 0.69473684, 0.69387755, 0.71287129, 0.72916667,\n",
       "        0.73469388, 0.76      , 0.70833333, 0.71287129, 0.69230769,\n",
       "        0.67368421, 0.74      , 0.7254902 , 0.72164948, 0.74226804,\n",
       "        0.76767677, 0.72164948, 0.70588235, 0.69902913, 0.68817204,\n",
       "        0.73267327, 0.7184466 , 0.70833333, 0.76      , 0.76767677,\n",
       "        0.72164948, 0.70103093, 0.71428571, 0.76767677, 0.72164948,\n",
       "        0.71428571, 0.74226804, 0.72916667, 0.72164948, 0.70212766,\n",
       "        0.70833333, 0.69387755, 0.71578947, 0.72727273, 0.69387755,\n",
       "        0.70833333, 0.70833333, 0.73469388, 0.66666667, 0.70833333,\n",
       "        0.72      , 0.65957447, 0.70833333, 0.72      , 0.71578947,\n",
       "        0.73469388, 0.75510204, 0.66666667, 0.72727273, 0.70588235,\n",
       "        0.69473684, 0.70103093, 0.74      , 0.71578947, 0.73469388,\n",
       "        0.75510204, 0.66666667, 0.72727273, 0.7184466 , 0.69387755,\n",
       "        0.72727273, 0.75247525, 0.71578947, 0.74226804, 0.7755102 ,\n",
       "        0.69387755, 0.71428571, 0.72727273, 0.72164948, 0.70833333,\n",
       "        0.74747475, 0.72164948, 0.73469388, 0.71428571, 0.69473684,\n",
       "        0.72916667, 0.72164948, 0.70833333, 0.70103093, 0.72164948,\n",
       "        0.70833333, 0.70833333, 0.70833333, 0.68041237, 0.70833333,\n",
       "        0.73267327, 0.67368421, 0.6875    , 0.69387755, 0.71578947,\n",
       "        0.70833333, 0.70833333, 0.69473684, 0.72164948, 0.72      ,\n",
       "        0.67368421, 0.68085106, 0.72      , 0.69473684, 0.70103093,\n",
       "        0.74747475, 0.68085106, 0.70833333, 0.72      , 0.68085106,\n",
       "        0.69473684, 0.72727273, 0.69473684, 0.70103093, 0.74747475,\n",
       "        0.70833333, 0.72164948, 0.72164948, 0.68817204, 0.72727273,\n",
       "        0.70833333, 0.70833333, 0.72164948, 0.70833333, 0.68085106,\n",
       "        0.69473684, 0.70833333, 0.68085106, 0.70103093, 0.71578947,\n",
       "        0.70212766, 0.70103093, 0.70833333, 0.69473684, 0.6875    ,\n",
       "        0.70833333, 0.6875    , 0.6875    , 0.71578947, 0.70103093,\n",
       "        0.70833333, 0.6875    , 0.68085106, 0.69473684, 0.69473684,\n",
       "        0.67368421, 0.67368421, 0.70833333, 0.70833333, 0.70103093,\n",
       "        0.6875    , 0.69473684, 0.69473684, 0.74747475, 0.66666667,\n",
       "        0.67368421, 0.6875    , 0.70103093, 0.70103093, 0.69473684,\n",
       "        0.67391304, 0.66666667, 0.70103093, 0.66666667, 0.65957447,\n",
       "        0.71428571, 0.66666667, 0.68041237, 0.6875    , 0.65957447,\n",
       "        0.67368421, 0.6875    , 0.65263158, 0.65957447, 0.70103093,\n",
       "        0.66666667, 0.65979381, 0.70103093, 0.66666667, 0.68041237,\n",
       "        0.69387755, 0.66666667, 0.6875    , 0.71428571, 0.6875    ,\n",
       "        0.67346939, 0.70103093, 0.69473684, 0.68686869, 0.69387755,\n",
       "        0.66666667, 0.66666667, 0.6875    , 0.6875    , 0.68041237,\n",
       "        0.6875    , 0.67368421, 0.68041237, 0.68041237, 0.65979381,\n",
       "        0.67346939, 0.6875    , 0.70103093, 0.67346939, 0.69387755,\n",
       "        0.72      , 0.72727273, 0.71428571, 0.70103093, 0.6875    ,\n",
       "        0.69387755, 0.70103093, 0.71428571, 0.72164948, 0.6875    ,\n",
       "        0.71428571, 0.70588235, 0.70103093, 0.6875    , 0.70707071,\n",
       "        0.68041237, 0.68817204, 0.72916667, 0.6875    , 0.68      ,\n",
       "        0.70588235, 0.6875    , 0.6875    , 0.68686869, 0.70212766,\n",
       "        0.71578947, 0.74226804, 0.68041237, 0.71287129, 0.69902913,\n",
       "        0.65957447, 0.68041237, 0.73076923, 0.70833333, 0.72916667,\n",
       "        0.75510204, 0.69387755, 0.71287129, 0.68627451, 0.67368421,\n",
       "        0.72727273, 0.74509804, 0.6875    , 0.70103093, 0.72727273,\n",
       "        0.71428571, 0.72727273, 0.72727273, 0.69387755, 0.70833333,\n",
       "        0.72164948, 0.6875    , 0.70103093, 0.70103093, 0.70103093,\n",
       "        0.72916667, 0.69387755, 0.69387755, 0.70833333, 0.70833333,\n",
       "        0.6875    , 0.6875    , 0.70833333, 0.66666667, 0.70103093,\n",
       "        0.66666667, 0.69387755, 0.72164948, 0.70103093, 0.6875    ,\n",
       "        0.6875    , 0.69473684, 0.69387755, 0.71428571, 0.70588235,\n",
       "        0.6875    , 0.70103093, 0.7254902 , 0.6875    , 0.68085106,\n",
       "        0.72164948, 0.65306122, 0.7254902 , 0.69306931, 0.67368421,\n",
       "        0.70103093, 0.7254902 , 0.6875    , 0.67368421, 0.70103093,\n",
       "        0.67346939, 0.71428571, 0.74      , 0.67346939, 0.71428571,\n",
       "        0.70833333, 0.6875    , 0.71428571, 0.72164948, 0.68041237,\n",
       "        0.70833333, 0.72164948, 0.66666667, 0.70103093, 0.72164948,\n",
       "        0.70103093, 0.69387755, 0.6875    , 0.66666667, 0.70103093,\n",
       "        0.68041237, 0.68686869, 0.71428571, 0.70103093, 0.6875    ,\n",
       "        0.66666667, 0.6875    , 0.66666667, 0.70103093, 0.70707071,\n",
       "        0.67346939, 0.6875    , 0.70103093, 0.6875    , 0.66666667,\n",
       "        0.70103093, 0.66666667, 0.70103093, 0.71287129, 0.68041237,\n",
       "        0.70103093, 0.69387755, 0.6875    , 0.69387755, 0.71428571,\n",
       "        0.65979381, 0.72727273, 0.71428571, 0.67346939, 0.71428571,\n",
       "        0.70103093, 0.68041237, 0.71428571, 0.71428571, 0.64583333,\n",
       "        0.6875    , 0.70833333, 0.64583333, 0.69387755, 0.70103093,\n",
       "        0.66666667, 0.70103093, 0.70103093, 0.64583333, 0.68041237,\n",
       "        0.70103093, 0.64583333, 0.69387755, 0.71428571, 0.66666667,\n",
       "        0.68041237, 0.68041237, 0.65979381, 0.67346939, 0.70103093,\n",
       "        0.64583333, 0.69387755, 0.66666667, 0.68041237, 0.68041237,\n",
       "        0.69387755, 0.64583333, 0.69387755, 0.71428571, 0.64583333,\n",
       "        0.70103093, 0.69387755, 0.68041237, 0.68041237, 0.69387755,\n",
       "        0.59340659, 0.65979381, 0.65979381, 0.59340659, 0.66666667,\n",
       "        0.6875    , 0.6       , 0.67346939, 0.68041237, 0.61702128,\n",
       "        0.64583333, 0.65979381, 0.60215054, 0.64583333, 0.67346939,\n",
       "        0.62365591, 0.65979381, 0.70103093, 0.61702128, 0.64583333,\n",
       "        0.67346939, 0.63157895, 0.64583333, 0.68686869, 0.65263158,\n",
       "        0.67368421, 0.70103093, 0.63157895, 0.65979381, 0.68      ,\n",
       "        0.63157895, 0.65979381, 0.69387755, 0.65263158, 0.6875    ,\n",
       "        0.70103093, 0.63157895, 0.65979381, 0.68      , 0.63829787,\n",
       "        0.66666667, 0.69387755, 0.65263158, 0.6875    , 0.6875    ]),\n",
       " 'split2_test_score': array([0.77669903, 0.75      , 0.74285714, 0.75      , 0.75      ,\n",
       "        0.75      , 0.76923077, 0.75728155, 0.76923077, 0.74285714,\n",
       "        0.74285714, 0.73584906, 0.76470588, 0.75      , 0.75      ,\n",
       "        0.74509804, 0.76470588, 0.74074074, 0.74509804, 0.73584906,\n",
       "        0.74285714, 0.76923077, 0.74285714, 0.75      , 0.75728155,\n",
       "        0.74509804, 0.74074074, 0.73786408, 0.73584906, 0.74285714,\n",
       "        0.75      , 0.75      , 0.74285714, 0.74509804, 0.75      ,\n",
       "        0.73394495, 0.72897196, 0.72897196, 0.72897196, 0.76923077,\n",
       "        0.74285714, 0.74285714, 0.75728155, 0.75      , 0.73394495,\n",
       "        0.77227723, 0.7254902 , 0.74285714, 0.76470588, 0.76470588,\n",
       "        0.75728155, 0.77669903, 0.75      , 0.75728155, 0.75728155,\n",
       "        0.74285714, 0.73584906, 0.75728155, 0.75      , 0.74285714,\n",
       "        0.75728155, 0.76470588, 0.75728155, 0.74509804, 0.74285714,\n",
       "        0.74285714, 0.75728155, 0.75      , 0.72897196, 0.75728155,\n",
       "        0.75728155, 0.73786408, 0.7254902 , 0.73584906, 0.74285714,\n",
       "        0.75      , 0.74285714, 0.74285714, 0.75728155, 0.74285714,\n",
       "        0.74074074, 0.73076923, 0.72897196, 0.72897196, 0.73786408,\n",
       "        0.74285714, 0.75      , 0.73786408, 0.75      , 0.74074074,\n",
       "        0.77227723, 0.73076923, 0.73584906, 0.77227723, 0.73076923,\n",
       "        0.75      , 0.75247525, 0.75728155, 0.75      , 0.77227723,\n",
       "        0.75247525, 0.73076923, 0.75728155, 0.73786408, 0.75      ,\n",
       "        0.75247525, 0.74509804, 0.74509804, 0.74509804, 0.75      ,\n",
       "        0.74285714, 0.73786408, 0.74509804, 0.74285714, 0.75      ,\n",
       "        0.74509804, 0.75728155, 0.71153846, 0.72380952, 0.73584906,\n",
       "        0.73786408, 0.73786408, 0.74285714, 0.73786408, 0.75      ,\n",
       "        0.75      , 0.72380952, 0.72222222, 0.73584906, 0.73786408,\n",
       "        0.75      , 0.74285714, 0.75      , 0.75728155, 0.76190476,\n",
       "        0.76      , 0.76470588, 0.75471698, 0.76      , 0.76470588,\n",
       "        0.75      , 0.74747475, 0.75247525, 0.74509804, 0.75247525,\n",
       "        0.75728155, 0.77669903, 0.76470588, 0.75728155, 0.75728155,\n",
       "        0.73786408, 0.74509804, 0.74509804, 0.73786408, 0.73786408,\n",
       "        0.74285714, 0.73786408, 0.75      , 0.75728155, 0.73076923,\n",
       "        0.74509804, 0.74509804, 0.73786408, 0.73076923, 0.72380952,\n",
       "        0.73786408, 0.74509804, 0.73786408, 0.73786408, 0.74509804,\n",
       "        0.73786408, 0.72380952, 0.71698113, 0.72897196, 0.72380952,\n",
       "        0.73076923, 0.75      , 0.72380952, 0.74509804, 0.75      ,\n",
       "        0.73469388, 0.74747475, 0.76      , 0.73469388, 0.74747475,\n",
       "        0.76      , 0.73469388, 0.76      , 0.77227723, 0.74747475,\n",
       "        0.76      , 0.76      , 0.74747475, 0.76      , 0.76470588,\n",
       "        0.74747475, 0.75      , 0.75728155, 0.74      , 0.74509804,\n",
       "        0.75728155, 0.76      , 0.74509804, 0.73786408, 0.74509804,\n",
       "        0.75      , 0.75      , 0.74      , 0.73786408, 0.72380952,\n",
       "        0.76767677, 0.74509804, 0.73786408, 0.73786408, 0.75      ,\n",
       "        0.75      , 0.74      , 0.73076923, 0.73786408, 0.76      ,\n",
       "        0.73076923, 0.73786408, 0.73076923, 0.75      , 0.75      ,\n",
       "        0.77669903, 0.75      , 0.74285714, 0.75      , 0.75      ,\n",
       "        0.75      , 0.76923077, 0.75728155, 0.76923077, 0.74285714,\n",
       "        0.74285714, 0.73584906, 0.76470588, 0.75      , 0.75      ,\n",
       "        0.74509804, 0.76470588, 0.74074074, 0.74509804, 0.73584906,\n",
       "        0.74285714, 0.76923077, 0.74285714, 0.75      , 0.75728155,\n",
       "        0.74509804, 0.74074074, 0.73786408, 0.73584906, 0.74285714,\n",
       "        0.75      , 0.75      , 0.74285714, 0.74509804, 0.75      ,\n",
       "        0.73394495, 0.72897196, 0.72897196, 0.72897196, 0.76923077,\n",
       "        0.74285714, 0.74285714, 0.75728155, 0.75      , 0.73394495,\n",
       "        0.77227723, 0.7254902 , 0.74285714, 0.76470588, 0.76470588,\n",
       "        0.75728155, 0.77669903, 0.75      , 0.75728155, 0.75728155,\n",
       "        0.74285714, 0.73584906, 0.75728155, 0.75      , 0.74285714,\n",
       "        0.75728155, 0.76470588, 0.75728155, 0.74509804, 0.74285714,\n",
       "        0.74285714, 0.75728155, 0.75      , 0.72897196, 0.75728155,\n",
       "        0.75728155, 0.73786408, 0.7254902 , 0.73584906, 0.74285714,\n",
       "        0.75      , 0.74285714, 0.74285714, 0.75728155, 0.74285714,\n",
       "        0.74074074, 0.73076923, 0.72897196, 0.72897196, 0.73786408,\n",
       "        0.74285714, 0.75      , 0.73786408, 0.75      , 0.74074074,\n",
       "        0.77227723, 0.73076923, 0.73584906, 0.77227723, 0.73076923,\n",
       "        0.75      , 0.75247525, 0.75728155, 0.75      , 0.77227723,\n",
       "        0.75247525, 0.73076923, 0.75728155, 0.73786408, 0.75      ,\n",
       "        0.75247525, 0.74509804, 0.74509804, 0.74509804, 0.75      ,\n",
       "        0.74285714, 0.73786408, 0.74509804, 0.74285714, 0.75      ,\n",
       "        0.74509804, 0.75728155, 0.71153846, 0.72380952, 0.73584906,\n",
       "        0.73786408, 0.73786408, 0.74285714, 0.73786408, 0.75      ,\n",
       "        0.75      , 0.72380952, 0.72222222, 0.73584906, 0.73786408,\n",
       "        0.75      , 0.74285714, 0.75      , 0.75728155, 0.76190476,\n",
       "        0.76      , 0.76470588, 0.75471698, 0.76      , 0.76470588,\n",
       "        0.75      , 0.74747475, 0.75247525, 0.74509804, 0.75247525,\n",
       "        0.75728155, 0.77669903, 0.76470588, 0.75728155, 0.75728155,\n",
       "        0.73786408, 0.74509804, 0.74509804, 0.73786408, 0.73786408,\n",
       "        0.74285714, 0.73786408, 0.75      , 0.75728155, 0.73076923,\n",
       "        0.74509804, 0.74509804, 0.73786408, 0.73076923, 0.72380952,\n",
       "        0.73786408, 0.74509804, 0.73786408, 0.73786408, 0.74509804,\n",
       "        0.73786408, 0.72380952, 0.71698113, 0.72897196, 0.72380952,\n",
       "        0.73076923, 0.75      , 0.72380952, 0.74509804, 0.75      ,\n",
       "        0.73469388, 0.74747475, 0.76      , 0.73469388, 0.74747475,\n",
       "        0.76      , 0.73469388, 0.76      , 0.77227723, 0.74747475,\n",
       "        0.76      , 0.76      , 0.74747475, 0.76      , 0.76470588,\n",
       "        0.74747475, 0.75      , 0.75728155, 0.74      , 0.74509804,\n",
       "        0.75728155, 0.76      , 0.74509804, 0.73786408, 0.74509804,\n",
       "        0.75      , 0.75      , 0.74      , 0.73786408, 0.72380952,\n",
       "        0.76767677, 0.74509804, 0.73786408, 0.73786408, 0.75      ,\n",
       "        0.75      , 0.74      , 0.73076923, 0.73786408, 0.76      ,\n",
       "        0.73076923, 0.73786408, 0.73076923, 0.75      , 0.75      ,\n",
       "        0.76923077, 0.71153846, 0.72222222, 0.76190476, 0.75728155,\n",
       "        0.75      , 0.75      , 0.75      , 0.75728155, 0.75728155,\n",
       "        0.7184466 , 0.73584906, 0.74285714, 0.75728155, 0.73584906,\n",
       "        0.76923077, 0.76923077, 0.76923077, 0.73584906, 0.74074074,\n",
       "        0.72222222, 0.75      , 0.74285714, 0.73584906, 0.75      ,\n",
       "        0.77358491, 0.76190476, 0.73786408, 0.71698113, 0.71559633,\n",
       "        0.74285714, 0.74285714, 0.74285714, 0.75      , 0.75      ,\n",
       "        0.74766355, 0.75471698, 0.72897196, 0.72897196, 0.73076923,\n",
       "        0.72897196, 0.72897196, 0.73786408, 0.74285714, 0.73394495,\n",
       "        0.77227723, 0.74074074, 0.74285714, 0.76      , 0.75471698,\n",
       "        0.74285714, 0.75      , 0.75      , 0.75728155, 0.73786408,\n",
       "        0.73076923, 0.74074074, 0.75      , 0.75      , 0.75      ,\n",
       "        0.75      , 0.75      , 0.75728155, 0.74509804, 0.71698113,\n",
       "        0.71698113, 0.74285714, 0.75      , 0.74285714, 0.75728155,\n",
       "        0.75728155, 0.75      , 0.71153846, 0.74766355, 0.72897196,\n",
       "        0.73786408, 0.74285714, 0.74285714, 0.74509804, 0.75728155,\n",
       "        0.75      , 0.7184466 , 0.72897196, 0.72222222, 0.73786408,\n",
       "        0.72897196, 0.72897196, 0.73076923, 0.75      , 0.75      ,\n",
       "        0.76      , 0.76635514, 0.75471698, 0.76      , 0.75471698,\n",
       "        0.74285714, 0.74      , 0.75728155, 0.74285714, 0.76      ,\n",
       "        0.73267327, 0.75728155, 0.74      , 0.75      , 0.75      ,\n",
       "        0.74509804, 0.74509804, 0.76470588, 0.76470588, 0.74766355,\n",
       "        0.73584906, 0.73267327, 0.75      , 0.74285714, 0.73786408,\n",
       "        0.75728155, 0.75      , 0.70588235, 0.74285714, 0.72897196,\n",
       "        0.73076923, 0.74285714, 0.74285714, 0.74509804, 0.75728155,\n",
       "        0.74285714, 0.70588235, 0.73394495, 0.74074074, 0.73786408,\n",
       "        0.74285714, 0.73584906, 0.73786408, 0.75      , 0.75      ,\n",
       "        0.74747475, 0.78431373, 0.75471698, 0.74747475, 0.75728155,\n",
       "        0.76190476, 0.74747475, 0.76923077, 0.75728155, 0.75247525,\n",
       "        0.76923077, 0.75471698, 0.74      , 0.75      , 0.75      ,\n",
       "        0.75247525, 0.76470588, 0.76923077, 0.73267327, 0.74766355,\n",
       "        0.74285714, 0.76      , 0.75728155, 0.75      , 0.74509804,\n",
       "        0.75728155, 0.75728155, 0.74747475, 0.7184466 , 0.74074074,\n",
       "        0.75247525, 0.73076923, 0.73786408, 0.74509804, 0.75      ,\n",
       "        0.75      , 0.75247525, 0.73786408, 0.74766355, 0.75247525,\n",
       "        0.73786408, 0.74285714, 0.73786408, 0.75      , 0.76190476,\n",
       "        0.71578947, 0.74747475, 0.76      , 0.72340426, 0.74747475,\n",
       "        0.75247525, 0.7628866 , 0.74      , 0.77227723, 0.74747475,\n",
       "        0.76470588, 0.76470588, 0.74226804, 0.75247525, 0.75247525,\n",
       "        0.75510204, 0.75247525, 0.75728155, 0.76767677, 0.76470588,\n",
       "        0.75728155, 0.76      , 0.75247525, 0.73786408, 0.74747475,\n",
       "        0.74509804, 0.75      , 0.75510204, 0.75      , 0.73584906,\n",
       "        0.76      , 0.75247525, 0.74509804, 0.74509804, 0.74509804,\n",
       "        0.75728155, 0.74747475, 0.75728155, 0.73584906, 0.74747475,\n",
       "        0.75247525, 0.73076923, 0.75247525, 0.74509804, 0.75728155]),\n",
       " 'split3_test_score': array([0.8       , 0.78846154, 0.77358491, 0.79245283, 0.78846154,\n",
       "        0.78095238, 0.8       , 0.78846154, 0.7961165 , 0.8       ,\n",
       "        0.77669903, 0.77358491, 0.79245283, 0.78846154, 0.77358491,\n",
       "        0.78846154, 0.7961165 , 0.78846154, 0.7961165 , 0.77669903,\n",
       "        0.76190476, 0.8       , 0.76190476, 0.76190476, 0.7961165 ,\n",
       "        0.78846154, 0.78095238, 0.78095238, 0.76923077, 0.76923077,\n",
       "        0.8       , 0.77669903, 0.78095238, 0.7961165 , 0.78846154,\n",
       "        0.78504673, 0.76923077, 0.76923077, 0.76923077, 0.78095238,\n",
       "        0.75728155, 0.76923077, 0.80769231, 0.78846154, 0.79245283,\n",
       "        0.76923077, 0.78095238, 0.78095238, 0.78095238, 0.78095238,\n",
       "        0.78846154, 0.78095238, 0.8       , 0.8       , 0.8       ,\n",
       "        0.77358491, 0.77358491, 0.79245283, 0.76923077, 0.78095238,\n",
       "        0.77669903, 0.80769231, 0.7961165 , 0.78846154, 0.76190476,\n",
       "        0.77358491, 0.80769231, 0.76923077, 0.77358491, 0.8       ,\n",
       "        0.7961165 , 0.7961165 , 0.80769231, 0.75471698, 0.76190476,\n",
       "        0.80769231, 0.77358491, 0.78095238, 0.80769231, 0.78846154,\n",
       "        0.8       , 0.8       , 0.76190476, 0.76923077, 0.8       ,\n",
       "        0.78095238, 0.78504673, 0.80769231, 0.7961165 , 0.78095238,\n",
       "        0.76923077, 0.78095238, 0.78846154, 0.76923077, 0.79245283,\n",
       "        0.78846154, 0.76923077, 0.8       , 0.8       , 0.78095238,\n",
       "        0.78846154, 0.77358491, 0.78095238, 0.78846154, 0.78846154,\n",
       "        0.77669903, 0.80769231, 0.7961165 , 0.8       , 0.79245283,\n",
       "        0.77358491, 0.7961165 , 0.78846154, 0.76923077, 0.7961165 ,\n",
       "        0.7961165 , 0.7961165 , 0.80769231, 0.76190476, 0.75471698,\n",
       "        0.80769231, 0.77358491, 0.76923077, 0.80769231, 0.7961165 ,\n",
       "        0.7961165 , 0.80769231, 0.76923077, 0.76190476, 0.80769231,\n",
       "        0.77358491, 0.79245283, 0.80769231, 0.7961165 , 0.7961165 ,\n",
       "        0.76190476, 0.78846154, 0.78095238, 0.76470588, 0.78095238,\n",
       "        0.79245283, 0.77227723, 0.78095238, 0.79245283, 0.77669903,\n",
       "        0.8       , 0.78095238, 0.77669903, 0.78095238, 0.8       ,\n",
       "        0.77669903, 0.8       , 0.80769231, 0.7961165 , 0.78095238,\n",
       "        0.78846154, 0.78095238, 0.8       , 0.78846154, 0.7961165 ,\n",
       "        0.80769231, 0.7961165 , 0.80769231, 0.7961165 , 0.76190476,\n",
       "        0.80769231, 0.8       , 0.77358491, 0.7961165 , 0.80769231,\n",
       "        0.7961165 , 0.80769231, 0.78095238, 0.76190476, 0.80769231,\n",
       "        0.78504673, 0.77358491, 0.7961165 , 0.80769231, 0.7961165 ,\n",
       "        0.75510204, 0.75728155, 0.76923077, 0.75510204, 0.75728155,\n",
       "        0.76190476, 0.73267327, 0.75728155, 0.76190476, 0.75247525,\n",
       "        0.75728155, 0.78846154, 0.75728155, 0.75728155, 0.78095238,\n",
       "        0.73786408, 0.75728155, 0.76923077, 0.7961165 , 0.77669903,\n",
       "        0.7961165 , 0.7961165 , 0.78846154, 0.78846154, 0.75471698,\n",
       "        0.78095238, 0.78846154, 0.77227723, 0.77669903, 0.7961165 ,\n",
       "        0.77227723, 0.77669903, 0.8       , 0.76923077, 0.78846154,\n",
       "        0.8       , 0.7961165 , 0.78846154, 0.78846154, 0.78431373,\n",
       "        0.78846154, 0.8       , 0.76923077, 0.78846154, 0.80769231,\n",
       "        0.8       , 0.78846154, 0.77358491, 0.79245283, 0.78846154,\n",
       "        0.78095238, 0.8       , 0.78846154, 0.7961165 , 0.8       ,\n",
       "        0.77669903, 0.77358491, 0.79245283, 0.78846154, 0.77358491,\n",
       "        0.78846154, 0.7961165 , 0.78846154, 0.7961165 , 0.77669903,\n",
       "        0.76190476, 0.8       , 0.76190476, 0.76190476, 0.7961165 ,\n",
       "        0.78846154, 0.78095238, 0.78095238, 0.76923077, 0.76923077,\n",
       "        0.8       , 0.77669903, 0.78095238, 0.7961165 , 0.78846154,\n",
       "        0.78504673, 0.76923077, 0.76923077, 0.76923077, 0.78095238,\n",
       "        0.75728155, 0.76923077, 0.80769231, 0.78846154, 0.79245283,\n",
       "        0.76923077, 0.78095238, 0.78095238, 0.78095238, 0.78095238,\n",
       "        0.78846154, 0.78095238, 0.8       , 0.8       , 0.8       ,\n",
       "        0.77358491, 0.77358491, 0.79245283, 0.76923077, 0.78095238,\n",
       "        0.77669903, 0.80769231, 0.7961165 , 0.78846154, 0.76190476,\n",
       "        0.77358491, 0.80769231, 0.76923077, 0.77358491, 0.8       ,\n",
       "        0.7961165 , 0.7961165 , 0.80769231, 0.75471698, 0.76190476,\n",
       "        0.80769231, 0.77358491, 0.78095238, 0.80769231, 0.78846154,\n",
       "        0.8       , 0.8       , 0.76190476, 0.76923077, 0.8       ,\n",
       "        0.78095238, 0.78504673, 0.80769231, 0.7961165 , 0.78095238,\n",
       "        0.76923077, 0.78095238, 0.78846154, 0.76923077, 0.79245283,\n",
       "        0.78846154, 0.76923077, 0.8       , 0.8       , 0.78095238,\n",
       "        0.78846154, 0.77358491, 0.78095238, 0.78846154, 0.78846154,\n",
       "        0.77669903, 0.80769231, 0.7961165 , 0.8       , 0.79245283,\n",
       "        0.77358491, 0.7961165 , 0.78846154, 0.76923077, 0.7961165 ,\n",
       "        0.7961165 , 0.7961165 , 0.80769231, 0.76190476, 0.75471698,\n",
       "        0.80769231, 0.77358491, 0.76923077, 0.80769231, 0.7961165 ,\n",
       "        0.7961165 , 0.80769231, 0.76923077, 0.76190476, 0.80769231,\n",
       "        0.77358491, 0.79245283, 0.80769231, 0.7961165 , 0.7961165 ,\n",
       "        0.76190476, 0.78846154, 0.78095238, 0.76470588, 0.78095238,\n",
       "        0.79245283, 0.77227723, 0.78095238, 0.79245283, 0.77669903,\n",
       "        0.8       , 0.78095238, 0.77669903, 0.78095238, 0.8       ,\n",
       "        0.77669903, 0.8       , 0.80769231, 0.7961165 , 0.78095238,\n",
       "        0.78846154, 0.78095238, 0.8       , 0.78846154, 0.7961165 ,\n",
       "        0.80769231, 0.7961165 , 0.80769231, 0.7961165 , 0.76190476,\n",
       "        0.80769231, 0.8       , 0.77358491, 0.7961165 , 0.80769231,\n",
       "        0.7961165 , 0.80769231, 0.78095238, 0.76190476, 0.80769231,\n",
       "        0.78504673, 0.77358491, 0.7961165 , 0.80769231, 0.7961165 ,\n",
       "        0.75510204, 0.75728155, 0.76923077, 0.75510204, 0.75728155,\n",
       "        0.76190476, 0.73267327, 0.75728155, 0.76190476, 0.75247525,\n",
       "        0.75728155, 0.78846154, 0.75728155, 0.75728155, 0.78095238,\n",
       "        0.73786408, 0.75728155, 0.76923077, 0.7961165 , 0.77669903,\n",
       "        0.7961165 , 0.7961165 , 0.78846154, 0.78846154, 0.75471698,\n",
       "        0.78095238, 0.78846154, 0.77227723, 0.77669903, 0.7961165 ,\n",
       "        0.77227723, 0.77669903, 0.8       , 0.76923077, 0.78846154,\n",
       "        0.8       , 0.7961165 , 0.78846154, 0.78846154, 0.78431373,\n",
       "        0.78846154, 0.8       , 0.76923077, 0.78846154, 0.80769231,\n",
       "        0.8       , 0.78504673, 0.79245283, 0.8       , 0.79245283,\n",
       "        0.8       , 0.81132075, 0.78846154, 0.81132075, 0.8       ,\n",
       "        0.79245283, 0.78095238, 0.80373832, 0.78846154, 0.78095238,\n",
       "        0.81904762, 0.80769231, 0.80769231, 0.81904762, 0.77358491,\n",
       "        0.77358491, 0.81132075, 0.78095238, 0.77358491, 0.83018868,\n",
       "        0.78846154, 0.7961165 , 0.80769231, 0.76923077, 0.78095238,\n",
       "        0.81904762, 0.78095238, 0.78095238, 0.81904762, 0.80769231,\n",
       "        0.8       , 0.8       , 0.76923077, 0.76190476, 0.80769231,\n",
       "        0.78095238, 0.77358491, 0.81904762, 0.8       , 0.8       ,\n",
       "        0.78846154, 0.8       , 0.78504673, 0.77669903, 0.78095238,\n",
       "        0.78846154, 0.77669903, 0.79245283, 0.78846154, 0.81132075,\n",
       "        0.78095238, 0.78846154, 0.8       , 0.78846154, 0.78095238,\n",
       "        0.78846154, 0.80769231, 0.80769231, 0.82242991, 0.78095238,\n",
       "        0.77358491, 0.80769231, 0.7961165 , 0.77358491, 0.81132075,\n",
       "        0.7961165 , 0.7961165 , 0.80769231, 0.76923077, 0.77358491,\n",
       "        0.81904762, 0.78504673, 0.78095238, 0.80769231, 0.7961165 ,\n",
       "        0.78846154, 0.80769231, 0.76923077, 0.76190476, 0.80769231,\n",
       "        0.79245283, 0.77358491, 0.81904762, 0.76923077, 0.78095238,\n",
       "        0.77669903, 0.8       , 0.79245283, 0.77669903, 0.79245283,\n",
       "        0.78846154, 0.76470588, 0.79245283, 0.7961165 , 0.76470588,\n",
       "        0.8       , 0.79245283, 0.77669903, 0.79245283, 0.77669903,\n",
       "        0.77669903, 0.81904762, 0.80769231, 0.7961165 , 0.80769231,\n",
       "        0.77358491, 0.78095238, 0.8       , 0.77358491, 0.78095238,\n",
       "        0.81904762, 0.7961165 , 0.80769231, 0.78095238, 0.77358491,\n",
       "        0.80769231, 0.79245283, 0.79245283, 0.80769231, 0.80769231,\n",
       "        0.8       , 0.80769231, 0.76923077, 0.77669903, 0.80769231,\n",
       "        0.79245283, 0.78095238, 0.80769231, 0.81904762, 0.79245283,\n",
       "        0.77227723, 0.78846154, 0.78095238, 0.77227723, 0.78095238,\n",
       "        0.79245283, 0.76470588, 0.78095238, 0.79245283, 0.76470588,\n",
       "        0.80373832, 0.79245283, 0.76470588, 0.79245283, 0.79245283,\n",
       "        0.76470588, 0.80373832, 0.81904762, 0.77669903, 0.80373832,\n",
       "        0.79245283, 0.76470588, 0.79245283, 0.79245283, 0.76923077,\n",
       "        0.79245283, 0.80769231, 0.77227723, 0.8       , 0.79245283,\n",
       "        0.77227723, 0.78846154, 0.79245283, 0.76470588, 0.8       ,\n",
       "        0.81904762, 0.78846154, 0.8       , 0.79245283, 0.78846154,\n",
       "        0.8       , 0.78504673, 0.78846154, 0.81132075, 0.81904762,\n",
       "        0.77083333, 0.78      , 0.75728155, 0.75510204, 0.78      ,\n",
       "        0.75728155, 0.7628866 , 0.78      , 0.75728155, 0.76767677,\n",
       "        0.77227723, 0.75728155, 0.75510204, 0.77227723, 0.75728155,\n",
       "        0.74226804, 0.77227723, 0.75728155, 0.78      , 0.77227723,\n",
       "        0.78095238, 0.78      , 0.76470588, 0.78095238, 0.76767677,\n",
       "        0.76      , 0.78095238, 0.78350515, 0.77227723, 0.78846154,\n",
       "        0.75510204, 0.76470588, 0.78095238, 0.75510204, 0.76      ,\n",
       "        0.78095238, 0.79591837, 0.77669903, 0.78846154, 0.7628866 ,\n",
       "        0.76923077, 0.79245283, 0.75510204, 0.77227723, 0.78846154]),\n",
       " 'split4_test_score': array([0.76470588, 0.80769231, 0.8       , 0.74      , 0.76767677,\n",
       "        0.78      , 0.71428571, 0.75510204, 0.7755102 , 0.78787879,\n",
       "        0.80412371, 0.8       , 0.76      , 0.75510204, 0.76767677,\n",
       "        0.73267327, 0.75789474, 0.76767677, 0.82      , 0.82828283,\n",
       "        0.79591837, 0.78431373, 0.78787879, 0.77227723, 0.76      ,\n",
       "        0.75510204, 0.76      , 0.78787879, 0.80808081, 0.76767677,\n",
       "        0.78787879, 0.78787879, 0.78      , 0.76      , 0.75510204,\n",
       "        0.77227723, 0.80392157, 0.80808081, 0.79207921, 0.79207921,\n",
       "        0.78787879, 0.78      , 0.78      , 0.75510204, 0.76767677,\n",
       "        0.74226804, 0.78846154, 0.80808081, 0.74509804, 0.76      ,\n",
       "        0.79207921, 0.74      , 0.74      , 0.7628866 , 0.76767677,\n",
       "        0.8125    , 0.7755102 , 0.74      , 0.76767677, 0.75510204,\n",
       "        0.72727273, 0.74226804, 0.7628866 , 0.78787879, 0.82      ,\n",
       "        0.8       , 0.76767677, 0.78350515, 0.78787879, 0.75510204,\n",
       "        0.7755102 , 0.78      , 0.78787879, 0.82      , 0.8       ,\n",
       "        0.79207921, 0.78787879, 0.7755102 , 0.76      , 0.7628866 ,\n",
       "        0.76767677, 0.8       , 0.83168317, 0.8       , 0.79207921,\n",
       "        0.80392157, 0.79207921, 0.77227723, 0.75510204, 0.76      ,\n",
       "        0.74226804, 0.80392157, 0.80392157, 0.73469388, 0.7961165 ,\n",
       "        0.76      , 0.73469388, 0.73267327, 0.74747475, 0.76      ,\n",
       "        0.77227723, 0.79591837, 0.72727273, 0.7755102 , 0.75510204,\n",
       "        0.74509804, 0.72727273, 0.7628866 , 0.78      , 0.79591837,\n",
       "        0.80808081, 0.74747475, 0.76      , 0.79591837, 0.76      ,\n",
       "        0.74747475, 0.7628866 , 0.8       , 0.83168317, 0.80808081,\n",
       "        0.78846154, 0.78431373, 0.8       , 0.76      , 0.75510204,\n",
       "        0.7628866 , 0.80392157, 0.82352941, 0.82      , 0.81188119,\n",
       "        0.78431373, 0.79207921, 0.77227723, 0.7628866 , 0.7628866 ,\n",
       "        0.74226804, 0.75247525, 0.80392157, 0.74226804, 0.7254902 ,\n",
       "        0.76      , 0.72164948, 0.73267327, 0.72340426, 0.75510204,\n",
       "        0.79207921, 0.78787879, 0.74      , 0.77669903, 0.77227723,\n",
       "        0.74509804, 0.73267327, 0.72727273, 0.76767677, 0.79207921,\n",
       "        0.82828283, 0.75510204, 0.78431373, 0.79591837, 0.76767677,\n",
       "        0.74      , 0.75      , 0.8       , 0.80392157, 0.82352941,\n",
       "        0.76767677, 0.7961165 , 0.80392157, 0.74747475, 0.76      ,\n",
       "        0.74226804, 0.8       , 0.8       , 0.83168317, 0.78787879,\n",
       "        0.81553398, 0.80392157, 0.7755102 , 0.78      , 0.7628866 ,\n",
       "        0.73913043, 0.75      , 0.74226804, 0.73913043, 0.75789474,\n",
       "        0.74226804, 0.7311828 , 0.73684211, 0.73469388, 0.73913043,\n",
       "        0.75      , 0.7628866 , 0.73913043, 0.74226804, 0.75510204,\n",
       "        0.7311828 , 0.75510204, 0.76      , 0.77083333, 0.78350515,\n",
       "        0.7755102 , 0.73684211, 0.7755102 , 0.77227723, 0.70967742,\n",
       "        0.74747475, 0.77227723, 0.77894737, 0.78      , 0.80392157,\n",
       "        0.77083333, 0.7755102 , 0.77227723, 0.7311828 , 0.75510204,\n",
       "        0.76      , 0.78723404, 0.77227723, 0.78846154, 0.77894737,\n",
       "        0.78      , 0.80392157, 0.71578947, 0.75510204, 0.78      ,\n",
       "        0.76470588, 0.80769231, 0.8       , 0.74      , 0.76767677,\n",
       "        0.78      , 0.71428571, 0.75510204, 0.7755102 , 0.78787879,\n",
       "        0.80412371, 0.8       , 0.76      , 0.75510204, 0.76767677,\n",
       "        0.73267327, 0.75789474, 0.76767677, 0.82      , 0.82828283,\n",
       "        0.79591837, 0.78431373, 0.78787879, 0.77227723, 0.76      ,\n",
       "        0.75510204, 0.76      , 0.78787879, 0.80808081, 0.76767677,\n",
       "        0.78787879, 0.78787879, 0.78      , 0.76      , 0.75510204,\n",
       "        0.77227723, 0.80392157, 0.80808081, 0.79207921, 0.79207921,\n",
       "        0.78787879, 0.78      , 0.78      , 0.75510204, 0.76767677,\n",
       "        0.74226804, 0.78846154, 0.80808081, 0.74509804, 0.76      ,\n",
       "        0.79207921, 0.74      , 0.74      , 0.7628866 , 0.76767677,\n",
       "        0.8125    , 0.7755102 , 0.74      , 0.76767677, 0.75510204,\n",
       "        0.72727273, 0.74226804, 0.7628866 , 0.78787879, 0.82      ,\n",
       "        0.8       , 0.76767677, 0.78350515, 0.78787879, 0.75510204,\n",
       "        0.7755102 , 0.78      , 0.78787879, 0.82      , 0.8       ,\n",
       "        0.79207921, 0.78787879, 0.7755102 , 0.76      , 0.7628866 ,\n",
       "        0.76767677, 0.8       , 0.83168317, 0.8       , 0.79207921,\n",
       "        0.80392157, 0.79207921, 0.77227723, 0.75510204, 0.76      ,\n",
       "        0.74226804, 0.80392157, 0.80392157, 0.73469388, 0.7961165 ,\n",
       "        0.76      , 0.73469388, 0.73267327, 0.74747475, 0.76      ,\n",
       "        0.77227723, 0.79591837, 0.72727273, 0.7755102 , 0.75510204,\n",
       "        0.74509804, 0.72727273, 0.7628866 , 0.78      , 0.79591837,\n",
       "        0.80808081, 0.74747475, 0.76      , 0.79591837, 0.76      ,\n",
       "        0.74747475, 0.7628866 , 0.8       , 0.83168317, 0.80808081,\n",
       "        0.78846154, 0.78431373, 0.8       , 0.76      , 0.75510204,\n",
       "        0.7628866 , 0.80392157, 0.82352941, 0.82      , 0.81188119,\n",
       "        0.78431373, 0.79207921, 0.77227723, 0.7628866 , 0.7628866 ,\n",
       "        0.74226804, 0.75247525, 0.80392157, 0.74226804, 0.7254902 ,\n",
       "        0.76      , 0.72164948, 0.73267327, 0.72340426, 0.75510204,\n",
       "        0.79207921, 0.78787879, 0.74      , 0.77669903, 0.77227723,\n",
       "        0.74509804, 0.73267327, 0.72727273, 0.76767677, 0.79207921,\n",
       "        0.82828283, 0.75510204, 0.78431373, 0.79591837, 0.76767677,\n",
       "        0.74      , 0.75      , 0.8       , 0.80392157, 0.82352941,\n",
       "        0.76767677, 0.7961165 , 0.80392157, 0.74747475, 0.76      ,\n",
       "        0.74226804, 0.8       , 0.8       , 0.83168317, 0.78787879,\n",
       "        0.81553398, 0.80392157, 0.7755102 , 0.78      , 0.7628866 ,\n",
       "        0.73913043, 0.75      , 0.74226804, 0.73913043, 0.75789474,\n",
       "        0.74226804, 0.7311828 , 0.73684211, 0.73469388, 0.73913043,\n",
       "        0.75      , 0.7628866 , 0.73913043, 0.74226804, 0.75510204,\n",
       "        0.7311828 , 0.75510204, 0.76      , 0.77083333, 0.78350515,\n",
       "        0.7755102 , 0.73684211, 0.7755102 , 0.77227723, 0.70967742,\n",
       "        0.74747475, 0.77227723, 0.77894737, 0.78      , 0.80392157,\n",
       "        0.77083333, 0.7755102 , 0.77227723, 0.7311828 , 0.75510204,\n",
       "        0.76      , 0.78723404, 0.77227723, 0.78846154, 0.77894737,\n",
       "        0.78      , 0.80392157, 0.71578947, 0.75510204, 0.78      ,\n",
       "        0.76      , 0.80392157, 0.8       , 0.74509804, 0.77227723,\n",
       "        0.76767677, 0.74509804, 0.73267327, 0.74747475, 0.78      ,\n",
       "        0.80412371, 0.81632653, 0.74      , 0.77227723, 0.78      ,\n",
       "        0.74      , 0.74747475, 0.74747475, 0.78431373, 0.80808081,\n",
       "        0.7755102 , 0.75247525, 0.76470588, 0.77227723, 0.74509804,\n",
       "        0.7755102 , 0.77227723, 0.78      , 0.79591837, 0.7755102 ,\n",
       "        0.76470588, 0.77227723, 0.77669903, 0.76      , 0.77227723,\n",
       "        0.78431373, 0.79207921, 0.79591837, 0.78787879, 0.78      ,\n",
       "        0.77227723, 0.77227723, 0.75247525, 0.74747475, 0.77227723,\n",
       "        0.74747475, 0.7961165 , 0.83168317, 0.73267327, 0.77669903,\n",
       "        0.74509804, 0.74      , 0.73786408, 0.75247525, 0.74747475,\n",
       "        0.8       , 0.80808081, 0.76      , 0.77227723, 0.76      ,\n",
       "        0.74747475, 0.7254902 , 0.73469388, 0.76470588, 0.8       ,\n",
       "        0.81632653, 0.74      , 0.76470588, 0.76470588, 0.74747475,\n",
       "        0.75247525, 0.77227723, 0.7961165 , 0.77227723, 0.79591837,\n",
       "        0.78431373, 0.76470588, 0.77227723, 0.76470588, 0.75728155,\n",
       "        0.78      , 0.80769231, 0.81188119, 0.78787879, 0.78431373,\n",
       "        0.76470588, 0.79207921, 0.78      , 0.77227723, 0.78      ,\n",
       "        0.75510204, 0.78846154, 0.7961165 , 0.75510204, 0.78095238,\n",
       "        0.75728155, 0.74226804, 0.7254902 , 0.73267327, 0.73469388,\n",
       "        0.77669903, 0.79591837, 0.72727273, 0.7961165 , 0.76      ,\n",
       "        0.76      , 0.73076923, 0.74      , 0.73267327, 0.77669903,\n",
       "        0.82474227, 0.74747475, 0.74509804, 0.77227723, 0.73469388,\n",
       "        0.75      , 0.74747475, 0.76470588, 0.80392157, 0.8       ,\n",
       "        0.74747475, 0.75728155, 0.78      , 0.74747475, 0.74509804,\n",
       "        0.74      , 0.78846154, 0.77227723, 0.8       , 0.77227723,\n",
       "        0.76470588, 0.77227723, 0.76      , 0.74509804, 0.75247525,\n",
       "        0.73333333, 0.76      , 0.78846154, 0.73913043, 0.74509804,\n",
       "        0.8       , 0.7311828 , 0.73267327, 0.7254902 , 0.74468085,\n",
       "        0.75728155, 0.77669903, 0.73684211, 0.73786408, 0.77669903,\n",
       "        0.72164948, 0.7184466 , 0.73076923, 0.74226804, 0.78095238,\n",
       "        0.79207921, 0.73469388, 0.76190476, 0.77669903, 0.72916667,\n",
       "        0.73786408, 0.75      , 0.74226804, 0.77669903, 0.81188119,\n",
       "        0.73469388, 0.75471698, 0.76923077, 0.72164948, 0.73786408,\n",
       "        0.75247525, 0.74226804, 0.76190476, 0.80392157, 0.75510204,\n",
       "        0.77358491, 0.76923077, 0.74226804, 0.73786408, 0.75247525,\n",
       "        0.68965517, 0.74725275, 0.72916667, 0.6744186 , 0.73913043,\n",
       "        0.72727273, 0.66666667, 0.72916667, 0.72727273, 0.68817204,\n",
       "        0.73469388, 0.74      , 0.70212766, 0.73469388, 0.74      ,\n",
       "        0.65217391, 0.74226804, 0.73267327, 0.69565217, 0.74747475,\n",
       "        0.74      , 0.70212766, 0.74747475, 0.74509804, 0.67391304,\n",
       "        0.74      , 0.75247525, 0.70967742, 0.74747475, 0.75247525,\n",
       "        0.7173913 , 0.73469388, 0.74509804, 0.67391304, 0.74      ,\n",
       "        0.76      , 0.70967742, 0.74747475, 0.77669903, 0.7032967 ,\n",
       "        0.74226804, 0.76470588, 0.68131868, 0.72727273, 0.75247525]),\n",
       " 'mean_test_score': array([0.78155639, 0.78443445, 0.77583743, 0.77111749, 0.77765161,\n",
       "        0.77907359, 0.77358221, 0.77445474, 0.78818719, 0.77290935,\n",
       "        0.77274226, 0.76460172, 0.77840962, 0.770205  , 0.76987138,\n",
       "        0.76563523, 0.78100533, 0.77800326, 0.7837393 , 0.77965078,\n",
       "        0.76237049, 0.78391369, 0.76817744, 0.76640095, 0.78498353,\n",
       "        0.77722012, 0.77532892, 0.77223649, 0.77219668, 0.75890046,\n",
       "        0.77486162, 0.78346458, 0.77285024, 0.77764973, 0.77804011,\n",
       "        0.78041659, 0.77562854, 0.7662427 , 0.75786221, 0.77638391,\n",
       "        0.77501194, 0.76972595, 0.78541872, 0.78100975, 0.77934056,\n",
       "        0.76674167, 0.7700608 , 0.77685426, 0.78138358, 0.78021883,\n",
       "        0.78349853, 0.78053291, 0.77691026, 0.78144045, 0.77175383,\n",
       "        0.77832886, 0.76499511, 0.76973222, 0.77538507, 0.76217687,\n",
       "        0.76646635, 0.77767684, 0.78106949, 0.76962101, 0.77916807,\n",
       "        0.78036533, 0.76874205, 0.7708413 , 0.77296092, 0.77818363,\n",
       "        0.78747771, 0.78807395, 0.77009461, 0.77786478, 0.76911914,\n",
       "        0.78145069, 0.77194414, 0.77873773, 0.78070169, 0.77665362,\n",
       "        0.78133136, 0.7720362 , 0.77859398, 0.76713939, 0.77563795,\n",
       "        0.78187455, 0.78679402, 0.7775984 , 0.7795711 , 0.78173769,\n",
       "        0.75953072, 0.77628281, 0.78197476, 0.76691721, 0.7780834 ,\n",
       "        0.78226418, 0.76530685, 0.77580353, 0.77542902, 0.76396953,\n",
       "        0.78155306, 0.77746132, 0.75714424, 0.77200192, 0.77447118,\n",
       "        0.75889737, 0.77243656, 0.77391547, 0.77079905, 0.78188993,\n",
       "        0.78398824, 0.75902791, 0.7670857 , 0.76348356, 0.77137149,\n",
       "        0.77248145, 0.77473492, 0.77079352, 0.78035841, 0.7740264 ,\n",
       "        0.77016788, 0.76839968, 0.77729137, 0.76704894, 0.77187847,\n",
       "        0.78272414, 0.77355192, 0.77496018, 0.77678153, 0.77853151,\n",
       "        0.76975786, 0.78180617, 0.77193157, 0.77704802, 0.78397355,\n",
       "        0.76015779, 0.77345843, 0.77923837, 0.76072616, 0.76998127,\n",
       "        0.77470625, 0.7572939 , 0.76842386, 0.76308846, 0.75797393,\n",
       "        0.77515615, 0.78332173, 0.75810776, 0.77182023, 0.78161867,\n",
       "        0.76035776, 0.7643879 , 0.77243656, 0.76662578, 0.77167913,\n",
       "        0.78446075, 0.75963064, 0.77636275, 0.78403921, 0.76711869,\n",
       "        0.77277376, 0.76881983, 0.77093806, 0.77765785, 0.77334513,\n",
       "        0.76538347, 0.77773703, 0.77949806, 0.7636143 , 0.77306128,\n",
       "        0.76582665, 0.77158437, 0.7688311 , 0.78430396, 0.76520946,\n",
       "        0.77576411, 0.77555031, 0.76729343, 0.77763804, 0.77162178,\n",
       "        0.74477841, 0.75163153, 0.76420292, 0.74332913, 0.75179204,\n",
       "        0.76538867, 0.73474545, 0.75425414, 0.75693174, 0.73973098,\n",
       "        0.75554009, 0.7694666 , 0.74351419, 0.75117175, 0.77005522,\n",
       "        0.73663766, 0.75178242, 0.76550865, 0.75642543, 0.76448986,\n",
       "        0.77425413, 0.75613558, 0.76901093, 0.77227468, 0.7368453 ,\n",
       "        0.75900675, 0.77035394, 0.75719229, 0.7636333 , 0.77089197,\n",
       "        0.76632413, 0.75845135, 0.76752826, 0.75250247, 0.76342264,\n",
       "        0.76812745, 0.76607362, 0.76004064, 0.76469647, 0.76568315,\n",
       "        0.7601966 , 0.77385713, 0.74902065, 0.76203404, 0.77494142,\n",
       "        0.78155639, 0.78443445, 0.77583743, 0.77111749, 0.77765161,\n",
       "        0.77907359, 0.77358221, 0.77445474, 0.78818719, 0.77290935,\n",
       "        0.77274226, 0.76460172, 0.77840962, 0.770205  , 0.76987138,\n",
       "        0.76563523, 0.78100533, 0.77800326, 0.7837393 , 0.77965078,\n",
       "        0.76237049, 0.78391369, 0.76817744, 0.76640095, 0.78498353,\n",
       "        0.77722012, 0.77532892, 0.77223649, 0.77219668, 0.75890046,\n",
       "        0.77486162, 0.78346458, 0.77285024, 0.77764973, 0.77804011,\n",
       "        0.78041659, 0.77562854, 0.7662427 , 0.75786221, 0.77638391,\n",
       "        0.77501194, 0.76972595, 0.78541872, 0.78100975, 0.77934056,\n",
       "        0.76674167, 0.7700608 , 0.77685426, 0.78138358, 0.78021883,\n",
       "        0.78349853, 0.78053291, 0.77691026, 0.78144045, 0.77175383,\n",
       "        0.77832886, 0.76499511, 0.76973222, 0.77538507, 0.76217687,\n",
       "        0.76646635, 0.77767684, 0.78106949, 0.76962101, 0.77916807,\n",
       "        0.78036533, 0.76874205, 0.7708413 , 0.77296092, 0.77818363,\n",
       "        0.78747771, 0.78807395, 0.77009461, 0.77786478, 0.76911914,\n",
       "        0.78145069, 0.77194414, 0.77873773, 0.78070169, 0.77665362,\n",
       "        0.78133136, 0.7720362 , 0.77859398, 0.76713939, 0.77563795,\n",
       "        0.78187455, 0.78679402, 0.7775984 , 0.7795711 , 0.78173769,\n",
       "        0.75953072, 0.77628281, 0.78197476, 0.76691721, 0.7780834 ,\n",
       "        0.78226418, 0.76530685, 0.77580353, 0.77542902, 0.76396953,\n",
       "        0.78155306, 0.77746132, 0.75714424, 0.77200192, 0.77447118,\n",
       "        0.75889737, 0.77243656, 0.77391547, 0.77079905, 0.78188993,\n",
       "        0.78398824, 0.75902791, 0.7670857 , 0.76348356, 0.77137149,\n",
       "        0.77248145, 0.77473492, 0.77079352, 0.78035841, 0.7740264 ,\n",
       "        0.77016788, 0.76839968, 0.77729137, 0.76704894, 0.77187847,\n",
       "        0.78272414, 0.77355192, 0.77496018, 0.77678153, 0.77853151,\n",
       "        0.76975786, 0.78180617, 0.77193157, 0.77704802, 0.78397355,\n",
       "        0.76015779, 0.77345843, 0.77923837, 0.76072616, 0.76998127,\n",
       "        0.77470625, 0.7572939 , 0.76842386, 0.76308846, 0.75797393,\n",
       "        0.77515615, 0.78332173, 0.75810776, 0.77182023, 0.78161867,\n",
       "        0.76035776, 0.7643879 , 0.77243656, 0.76662578, 0.77167913,\n",
       "        0.78446075, 0.75963064, 0.77636275, 0.78403921, 0.76711869,\n",
       "        0.77277376, 0.76881983, 0.77093806, 0.77765785, 0.77334513,\n",
       "        0.76538347, 0.77773703, 0.77949806, 0.7636143 , 0.77306128,\n",
       "        0.76582665, 0.77158437, 0.7688311 , 0.78430396, 0.76520946,\n",
       "        0.77576411, 0.77555031, 0.76729343, 0.77763804, 0.77162178,\n",
       "        0.74477841, 0.75163153, 0.76420292, 0.74332913, 0.75179204,\n",
       "        0.76538867, 0.73474545, 0.75425414, 0.75693174, 0.73973098,\n",
       "        0.75554009, 0.7694666 , 0.74351419, 0.75117175, 0.77005522,\n",
       "        0.73663766, 0.75178242, 0.76550865, 0.75642543, 0.76448986,\n",
       "        0.77425413, 0.75613558, 0.76901093, 0.77227468, 0.7368453 ,\n",
       "        0.75900675, 0.77035394, 0.75719229, 0.7636333 , 0.77089197,\n",
       "        0.76632413, 0.75845135, 0.76752826, 0.75250247, 0.76342264,\n",
       "        0.76812745, 0.76607362, 0.76004064, 0.76469647, 0.76568315,\n",
       "        0.7601966 , 0.77385713, 0.74902065, 0.76203404, 0.77494142,\n",
       "        0.77455204, 0.77585293, 0.77379215, 0.77360675, 0.7766596 ,\n",
       "        0.77373944, 0.77236373, 0.76851268, 0.77897388, 0.76806311,\n",
       "        0.77189951, 0.77161159, 0.76675605, 0.77253264, 0.772203  ,\n",
       "        0.76872844, 0.7755909 , 0.77994367, 0.77233237, 0.76358809,\n",
       "        0.75622425, 0.77113299, 0.76383053, 0.7575255 , 0.77635666,\n",
       "        0.78154301, 0.7815036 , 0.76657837, 0.76661936, 0.75344091,\n",
       "        0.76978604, 0.76617361, 0.77712934, 0.77547619, 0.78490416,\n",
       "        0.78664663, 0.77843178, 0.76773511, 0.75454447, 0.77097817,\n",
       "        0.77444388, 0.77824385, 0.77025118, 0.76649686, 0.7720836 ,\n",
       "        0.77083648, 0.77981629, 0.784991  , 0.76064997, 0.77668936,\n",
       "        0.77269016, 0.75946726, 0.77102685, 0.77292678, 0.76424398,\n",
       "        0.77905144, 0.77385118, 0.76711214, 0.77304519, 0.77073093,\n",
       "        0.75939314, 0.76501029, 0.772474  , 0.76448598, 0.76678318,\n",
       "        0.76170214, 0.76322203, 0.77512183, 0.76506322, 0.76542129,\n",
       "        0.76790543, 0.77570304, 0.77107573, 0.76768174, 0.76625613,\n",
       "        0.77661887, 0.76735559, 0.77294284, 0.76798954, 0.76973471,\n",
       "        0.77564125, 0.76537849, 0.77574227, 0.75612181, 0.77325988,\n",
       "        0.76772935, 0.77657427, 0.77433716, 0.76446701, 0.77327045,\n",
       "        0.75383841, 0.77852636, 0.78427631, 0.76335112, 0.77877861,\n",
       "        0.77193573, 0.75552224, 0.76652951, 0.7717362 , 0.75266831,\n",
       "        0.77053142, 0.78433423, 0.75012768, 0.77879384, 0.77090047,\n",
       "        0.76127148, 0.76638594, 0.76921041, 0.75357093, 0.77524461,\n",
       "        0.77321475, 0.7542997 , 0.77275054, 0.76718081, 0.75130886,\n",
       "        0.76558946, 0.7638373 , 0.74977376, 0.77437986, 0.77055297,\n",
       "        0.75821777, 0.76464576, 0.77025847, 0.76065981, 0.7662215 ,\n",
       "        0.76820619, 0.76007721, 0.76392423, 0.77468966, 0.77219822,\n",
       "        0.77050639, 0.76459124, 0.76331716, 0.77468156, 0.77107353,\n",
       "        0.74657582, 0.77671548, 0.77467361, 0.75554252, 0.76815099,\n",
       "        0.78137473, 0.74802047, 0.7686592 , 0.76877585, 0.74153906,\n",
       "        0.76988676, 0.77731422, 0.73909243, 0.76346634, 0.77491034,\n",
       "        0.73463481, 0.76788138, 0.77431274, 0.73949473, 0.76888996,\n",
       "        0.77823304, 0.74431192, 0.76973079, 0.77923653, 0.73713447,\n",
       "        0.75830805, 0.77162627, 0.75001933, 0.76005964, 0.77784859,\n",
       "        0.74600439, 0.76386209, 0.76411666, 0.74155683, 0.76165529,\n",
       "        0.7756291 , 0.74907294, 0.76902631, 0.78196176, 0.75332289,\n",
       "        0.77279301, 0.77075146, 0.74898488, 0.76454689, 0.77853796,\n",
       "        0.70504803, 0.74940426, 0.74362464, 0.70207529, 0.7508649 ,\n",
       "        0.75290591, 0.70680258, 0.74779252, 0.75667955, 0.71726046,\n",
       "        0.74268574, 0.75235625, 0.71588521, 0.74188068, 0.75434221,\n",
       "        0.6968622 , 0.73794019, 0.75435934, 0.72526153, 0.75005824,\n",
       "        0.7566773 , 0.73561089, 0.74536315, 0.75815664, 0.71887686,\n",
       "        0.74294012, 0.76322834, 0.73176219, 0.74752532, 0.75535717,\n",
       "        0.73026127, 0.74559907, 0.7610052 , 0.71752285, 0.74322064,\n",
       "        0.75985297, 0.7269299 , 0.75390639, 0.76020192, 0.72691292,\n",
       "        0.7510766 , 0.76201766, 0.72215166, 0.74561327, 0.7634803 ]),\n",
       " 'std_test_score': array([0.03273069, 0.04599057, 0.0547373 , 0.04642303, 0.05482315,\n",
       "        0.04388121, 0.05455261, 0.04758839, 0.04336797, 0.04544344,\n",
       "        0.05196629, 0.03863991, 0.04263694, 0.03580744, 0.03887191,\n",
       "        0.04689809, 0.04361717, 0.03641738, 0.0585227 , 0.05019065,\n",
       "        0.05516163, 0.05667653, 0.05294913, 0.03971985, 0.05312597,\n",
       "        0.04641209, 0.03241223, 0.04700957, 0.04488938, 0.0422178 ,\n",
       "        0.0622573 , 0.04326594, 0.03772502, 0.05009109, 0.04129586,\n",
       "        0.03561877, 0.04925861, 0.04377258, 0.03839692, 0.0524212 ,\n",
       "        0.04382682, 0.04032782, 0.05489659, 0.03767369, 0.03345849,\n",
       "        0.03593443, 0.05351025, 0.04430119, 0.03545541, 0.05066606,\n",
       "        0.04956074, 0.0444541 , 0.05045162, 0.04877281, 0.04350458,\n",
       "        0.05124428, 0.05035532, 0.04439227, 0.04623476, 0.04733755,\n",
       "        0.05361763, 0.05445022, 0.04158462, 0.06333664, 0.05529273,\n",
       "        0.05041946, 0.06389614, 0.04411639, 0.04814579, 0.04997856,\n",
       "        0.04767595, 0.04617896, 0.06782458, 0.04910404, 0.04478427,\n",
       "        0.05642483, 0.05084327, 0.04128784, 0.05029965, 0.04306147,\n",
       "        0.03658008, 0.0672276 , 0.04971728, 0.03900358, 0.05508906,\n",
       "        0.04526825, 0.03775959, 0.04945065, 0.04179275, 0.03756962,\n",
       "        0.04128302, 0.04971162, 0.04665737, 0.03999018, 0.05441136,\n",
       "        0.04403269, 0.04459091, 0.04615469, 0.0526542 , 0.03863129,\n",
       "        0.04637101, 0.0517326 , 0.03698051, 0.05241084, 0.0464639 ,\n",
       "        0.0343877 , 0.06068138, 0.05034438, 0.05621873, 0.05152372,\n",
       "        0.0473631 , 0.05620849, 0.05466779, 0.04257872, 0.04081701,\n",
       "        0.05418446, 0.04655117, 0.05709616, 0.05728324, 0.04880323,\n",
       "        0.05902526, 0.0604705 , 0.04688203, 0.04978578, 0.05222449,\n",
       "        0.04104733, 0.06201488, 0.05562067, 0.04858247, 0.06149663,\n",
       "        0.04915566, 0.04466579, 0.04835385, 0.05475766, 0.03733395,\n",
       "        0.039115  , 0.03963317, 0.03907409, 0.05161929, 0.04603162,\n",
       "        0.05157095, 0.04538594, 0.0474633 , 0.05031648, 0.04646517,\n",
       "        0.04665978, 0.048993  , 0.04818755, 0.04564168, 0.04885164,\n",
       "        0.04634543, 0.05071825, 0.06068138, 0.04854137, 0.05750185,\n",
       "        0.05357197, 0.04918693, 0.0569363 , 0.04843405, 0.04866569,\n",
       "        0.05532952, 0.05932907, 0.05420968, 0.05889785, 0.06208822,\n",
       "        0.05749608, 0.06625892, 0.05713068, 0.04293706, 0.05194328,\n",
       "        0.06049999, 0.05274671, 0.05679524, 0.04832773, 0.06218835,\n",
       "        0.06887819, 0.05799551, 0.04993215, 0.05239078, 0.05272984,\n",
       "        0.04707405, 0.0538766 , 0.0482042 , 0.04929275, 0.0562234 ,\n",
       "        0.04492711, 0.04492886, 0.05020161, 0.04618762, 0.0453435 ,\n",
       "        0.05166539, 0.05187702, 0.05384478, 0.05640146, 0.04751109,\n",
       "        0.04256625, 0.05607714, 0.04431303, 0.05064044, 0.05133231,\n",
       "        0.05047416, 0.0533002 , 0.05282524, 0.04607996, 0.0349273 ,\n",
       "        0.05493688, 0.04561167, 0.03668677, 0.04967747, 0.0533113 ,\n",
       "        0.05950718, 0.05310962, 0.0521797 , 0.04953592, 0.05314409,\n",
       "        0.05203417, 0.05493604, 0.05062006, 0.05098347, 0.06019372,\n",
       "        0.05331866, 0.05424989, 0.04567577, 0.05534024, 0.05087692,\n",
       "        0.03273069, 0.04599057, 0.0547373 , 0.04642303, 0.05482315,\n",
       "        0.04388121, 0.05455261, 0.04758839, 0.04336797, 0.04544344,\n",
       "        0.05196629, 0.03863991, 0.04263694, 0.03580744, 0.03887191,\n",
       "        0.04689809, 0.04361717, 0.03641738, 0.0585227 , 0.05019065,\n",
       "        0.05516163, 0.05667653, 0.05294913, 0.03971985, 0.05312597,\n",
       "        0.04641209, 0.03241223, 0.04700957, 0.04488938, 0.0422178 ,\n",
       "        0.0622573 , 0.04326594, 0.03772502, 0.05009109, 0.04129586,\n",
       "        0.03561877, 0.04925861, 0.04377258, 0.03839692, 0.0524212 ,\n",
       "        0.04382682, 0.04032782, 0.05489659, 0.03767369, 0.03345849,\n",
       "        0.03593443, 0.05351025, 0.04430119, 0.03545541, 0.05066606,\n",
       "        0.04956074, 0.0444541 , 0.05045162, 0.04877281, 0.04350458,\n",
       "        0.05124428, 0.05035532, 0.04439227, 0.04623476, 0.04733755,\n",
       "        0.05361763, 0.05445022, 0.04158462, 0.06333664, 0.05529273,\n",
       "        0.05041946, 0.06389614, 0.04411639, 0.04814579, 0.04997856,\n",
       "        0.04767595, 0.04617896, 0.06782458, 0.04910404, 0.04478427,\n",
       "        0.05642483, 0.05084327, 0.04128784, 0.05029965, 0.04306147,\n",
       "        0.03658008, 0.0672276 , 0.04971728, 0.03900358, 0.05508906,\n",
       "        0.04526825, 0.03775959, 0.04945065, 0.04179275, 0.03756962,\n",
       "        0.04128302, 0.04971162, 0.04665737, 0.03999018, 0.05441136,\n",
       "        0.04403269, 0.04459091, 0.04615469, 0.0526542 , 0.03863129,\n",
       "        0.04637101, 0.0517326 , 0.03698051, 0.05241084, 0.0464639 ,\n",
       "        0.0343877 , 0.06068138, 0.05034438, 0.05621873, 0.05152372,\n",
       "        0.0473631 , 0.05620849, 0.05466779, 0.04257872, 0.04081701,\n",
       "        0.05418446, 0.04655117, 0.05709616, 0.05728324, 0.04880323,\n",
       "        0.05902526, 0.0604705 , 0.04688203, 0.04978578, 0.05222449,\n",
       "        0.04104733, 0.06201488, 0.05562067, 0.04858247, 0.06149663,\n",
       "        0.04915566, 0.04466579, 0.04835385, 0.05475766, 0.03733395,\n",
       "        0.039115  , 0.03963317, 0.03907409, 0.05161929, 0.04603162,\n",
       "        0.05157095, 0.04538594, 0.0474633 , 0.05031648, 0.04646517,\n",
       "        0.04665978, 0.048993  , 0.04818755, 0.04564168, 0.04885164,\n",
       "        0.04634543, 0.05071825, 0.06068138, 0.04854137, 0.05750185,\n",
       "        0.05357197, 0.04918693, 0.0569363 , 0.04843405, 0.04866569,\n",
       "        0.05532952, 0.05932907, 0.05420968, 0.05889785, 0.06208822,\n",
       "        0.05749608, 0.06625892, 0.05713068, 0.04293706, 0.05194328,\n",
       "        0.06049999, 0.05274671, 0.05679524, 0.04832773, 0.06218835,\n",
       "        0.06887819, 0.05799551, 0.04993215, 0.05239078, 0.05272984,\n",
       "        0.04707405, 0.0538766 , 0.0482042 , 0.04929275, 0.0562234 ,\n",
       "        0.04492711, 0.04492886, 0.05020161, 0.04618762, 0.0453435 ,\n",
       "        0.05166539, 0.05187702, 0.05384478, 0.05640146, 0.04751109,\n",
       "        0.04256625, 0.05607714, 0.04431303, 0.05064044, 0.05133231,\n",
       "        0.05047416, 0.0533002 , 0.05282524, 0.04607996, 0.0349273 ,\n",
       "        0.05493688, 0.04561167, 0.03668677, 0.04967747, 0.0533113 ,\n",
       "        0.05950718, 0.05310962, 0.0521797 , 0.04953592, 0.05314409,\n",
       "        0.05203417, 0.05493604, 0.05062006, 0.05098347, 0.06019372,\n",
       "        0.05331866, 0.05424989, 0.04567577, 0.05534024, 0.05087692,\n",
       "        0.03539035, 0.05116443, 0.04820586, 0.05362337, 0.0600453 ,\n",
       "        0.05408139, 0.05398968, 0.05063795, 0.04879661, 0.04477185,\n",
       "        0.04697878, 0.04457835, 0.05153355, 0.05454508, 0.05082509,\n",
       "        0.05575448, 0.05927299, 0.0421926 , 0.05435914, 0.04957194,\n",
       "        0.03643849, 0.05715325, 0.0506946 , 0.04409704, 0.05686039,\n",
       "        0.04421478, 0.03187124, 0.05246155, 0.04759593, 0.03862123,\n",
       "        0.06923244, 0.0564495 , 0.04313838, 0.04787171, 0.04791829,\n",
       "        0.03532593, 0.05234509, 0.04335553, 0.04313881, 0.0647045 ,\n",
       "        0.0492686 , 0.04947873, 0.05944671, 0.04878066, 0.03807703,\n",
       "        0.03939238, 0.04000173, 0.04495972, 0.04857093, 0.05015794,\n",
       "        0.05118102, 0.05088318, 0.05909302, 0.05407741, 0.04626874,\n",
       "        0.04676504, 0.05099904, 0.04681398, 0.04539126, 0.04803786,\n",
       "        0.04551337, 0.05933248, 0.05238183, 0.05794512, 0.05050671,\n",
       "        0.06248551, 0.04985415, 0.04163115, 0.04639299, 0.04887407,\n",
       "        0.0524112 , 0.05601322, 0.05849578, 0.03949834, 0.04392981,\n",
       "        0.05890431, 0.04703614, 0.04037017, 0.05113429, 0.05752677,\n",
       "        0.03913168, 0.06923447, 0.04605049, 0.04403761, 0.06400556,\n",
       "        0.05212723, 0.05010053, 0.05979136, 0.05852062, 0.04988251,\n",
       "        0.04365701, 0.03704079, 0.03446061, 0.0567517 , 0.04520231,\n",
       "        0.05220157, 0.05060508, 0.04698921, 0.05335357, 0.04639818,\n",
       "        0.04551531, 0.04419545, 0.0572254 , 0.05113588, 0.04166054,\n",
       "        0.04001545, 0.05593426, 0.05468323, 0.0506997 , 0.04885236,\n",
       "        0.06130245, 0.04596595, 0.0492026 , 0.04747878, 0.04366233,\n",
       "        0.05960979, 0.05068092, 0.05539451, 0.04902497, 0.04879298,\n",
       "        0.05638443, 0.05179689, 0.04542236, 0.04688625, 0.06313016,\n",
       "        0.0545367 , 0.0630284 , 0.04738557, 0.04539552, 0.06174471,\n",
       "        0.05032727, 0.04866165, 0.04899342, 0.0603474 , 0.04496437,\n",
       "        0.05245753, 0.03197261, 0.03976912, 0.05552985, 0.04321543,\n",
       "        0.04943542, 0.04426913, 0.04562528, 0.05071827, 0.05146541,\n",
       "        0.04880906, 0.04782464, 0.05313867, 0.05072887, 0.05040496,\n",
       "        0.03768981, 0.05513288, 0.05522477, 0.05267206, 0.0521186 ,\n",
       "        0.0543099 , 0.05592615, 0.04873345, 0.04945336, 0.03897559,\n",
       "        0.04879844, 0.06098662, 0.05445694, 0.0569764 , 0.05081743,\n",
       "        0.05847043, 0.05359018, 0.06190358, 0.03908591, 0.05460859,\n",
       "        0.05890778, 0.05794964, 0.05371772, 0.04729104, 0.05982197,\n",
       "        0.05158125, 0.05542055, 0.0415503 , 0.05721266, 0.05883313,\n",
       "        0.06276121, 0.05090296, 0.04969962, 0.06273879, 0.05101707,\n",
       "        0.05007935, 0.06405857, 0.04830281, 0.05464566, 0.05782756,\n",
       "        0.05222741, 0.05758243, 0.06195087, 0.0532675 , 0.05589468,\n",
       "        0.0510153 , 0.04034404, 0.04028842, 0.06171576, 0.05739267,\n",
       "        0.05180196, 0.06202219, 0.05544577, 0.05075878, 0.04637382,\n",
       "        0.03976787, 0.04280033, 0.05652069, 0.0489542 , 0.04762808,\n",
       "        0.05413167, 0.0507862 , 0.04827062, 0.04508706, 0.03169567,\n",
       "        0.03323744, 0.05496032, 0.05470154, 0.04831559, 0.05143786,\n",
       "        0.05091324, 0.04678491, 0.0463078 , 0.03729708, 0.04734603]),\n",
       " 'rank_test_score': array([ 57,  18, 182, 323, 143, 105, 247, 229,   1, 265, 273, 479, 120,\n",
       "        350, 363, 457,  76, 132,  32,  92, 516,  30, 400, 441,  13, 156,\n",
       "        204, 288, 292, 553, 217,  36, 267, 145, 130,  82, 194, 447, 566,\n",
       "        174, 210, 372,  10,  74,  98, 431, 356, 163,  67,  88,  34,  80,\n",
       "        161,  65, 309, 122, 474, 369, 202, 518, 439, 139,  72, 374, 103,\n",
       "         84, 390, 334, 261, 126,   5,   3, 354, 134, 379,  63, 299, 111,\n",
       "         78, 170,  70, 295, 113, 418, 191,  49,   7, 149,  94,  53, 545,\n",
       "        179,  44, 427, 128,  42, 468, 184, 200, 493,  59, 151, 573, 297,\n",
       "        227, 555, 280, 239, 337,  47,  26, 549, 423, 504, 320, 277, 219,\n",
       "        339,  86, 237, 352, 397, 154, 425, 305,  40, 249, 212, 165, 116,\n",
       "        366,  51, 302, 159,  28, 536, 251, 100, 527, 360, 221, 569, 395,\n",
       "        514, 564, 207,  38, 562, 307,  55, 531, 487, 280, 433, 312,  16,\n",
       "        543, 176,  24, 420, 270, 387, 329, 141, 253, 465, 137,  96, 501,\n",
       "        258, 453, 318, 385,  21, 470, 186, 197, 415, 147, 315, 634, 610,\n",
       "        490, 640, 606, 463, 659, 594, 575, 648, 586, 376, 638, 613, 358,\n",
       "        656, 608, 460, 579, 483, 235, 582, 382, 286, 654, 551, 346, 571,\n",
       "        499, 332, 444, 557, 412, 603, 508, 403, 451, 540, 476, 455, 534,\n",
       "        241, 623, 520, 214,  57,  18, 182, 323, 143, 105, 247, 229,   1,\n",
       "        265, 273, 479, 120, 350, 363, 457,  76, 132,  32,  92, 516,  30,\n",
       "        400, 441,  13, 156, 204, 288, 292, 553, 217,  36, 267, 145, 130,\n",
       "         82, 194, 447, 566, 174, 210, 372,  10,  74,  98, 431, 356, 163,\n",
       "         67,  88,  34,  80, 161,  65, 309, 122, 474, 369, 202, 518, 439,\n",
       "        139,  72, 374, 103,  84, 390, 334, 261, 126,   5,   3, 354, 134,\n",
       "        379,  63, 299, 111,  78, 170,  70, 295, 113, 418, 191,  49,   7,\n",
       "        149,  94,  53, 545, 179,  44, 427, 128,  42, 468, 184, 200, 493,\n",
       "         59, 151, 573, 297, 227, 555, 280, 239, 337,  47,  26, 549, 423,\n",
       "        504, 320, 277, 219, 339,  86, 237, 352, 397, 154, 425, 305,  40,\n",
       "        249, 212, 165, 116, 366,  51, 302, 159,  28, 536, 251, 100, 527,\n",
       "        360, 221, 569, 395, 514, 564, 207,  38, 562, 307,  55, 531, 487,\n",
       "        280, 433, 312,  16, 543, 176,  24, 420, 270, 387, 329, 141, 253,\n",
       "        465, 137,  96, 501, 258, 453, 318, 385,  21, 470, 186, 197, 415,\n",
       "        147, 315, 634, 610, 490, 640, 606, 463, 659, 594, 575, 648, 586,\n",
       "        376, 638, 613, 358, 656, 608, 460, 579, 483, 235, 582, 382, 286,\n",
       "        654, 551, 346, 571, 499, 332, 444, 557, 412, 603, 508, 403, 451,\n",
       "        540, 476, 455, 534, 241, 623, 520, 214, 226, 181, 244, 246, 169,\n",
       "        245, 284, 394, 108, 405, 304, 317, 430, 276, 290, 392, 196,  90,\n",
       "        285, 503, 581, 322, 498, 568, 178,  61,  62, 436, 435, 599, 365,\n",
       "        450, 158, 199,  15,   9, 119, 409, 590, 328, 231, 124, 349, 438,\n",
       "        294, 336,  91,  12, 530, 168, 275, 547, 327, 264, 489, 107, 243,\n",
       "        422, 260, 342, 548, 473, 279, 485, 429, 523, 513, 209, 472, 462,\n",
       "        407, 189, 325, 411, 446, 172, 414, 263, 406, 368, 190, 467, 188,\n",
       "        584, 256, 410, 173, 233, 486, 255, 597, 118,  23, 510, 110, 301,\n",
       "        588, 437, 311, 602, 344,  20, 617, 109, 331, 525, 443, 378, 598,\n",
       "        206, 257, 593, 272, 417, 612, 459, 497, 620, 232, 343, 560, 478,\n",
       "        348, 529, 449, 399, 538, 495, 223, 291, 345, 481, 511, 224, 326,\n",
       "        629, 167, 225, 585, 402,  69, 626, 393, 389, 647, 362, 153, 651,\n",
       "        507, 216, 661, 408, 234, 650, 384, 125, 636, 371, 102, 653, 559,\n",
       "        314, 619, 539, 136, 630, 496, 492, 646, 524, 193, 622, 381,  46,\n",
       "        600, 269, 341, 625, 482, 115, 673, 621, 637, 674, 616, 601, 672,\n",
       "        627, 577, 670, 644, 605, 671, 645, 592, 675, 652, 591, 666, 618,\n",
       "        578, 658, 633, 561, 668, 643, 512, 662, 628, 589, 663, 632, 526,\n",
       "        669, 642, 542, 664, 596, 533, 665, 615, 522, 667, 631, 506],\n",
       "       dtype=int32)}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.5,\n",
       " 'learning_rate': 0.1,\n",
       " 'max_depth': 3,\n",
       " 'min_child_weight': 3,\n",
       " 'n_estimators': 500}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7881871942204813"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.793722\n",
      "F1: 0.680556\n"
     ]
    }
   ],
   "source": [
    "preds = gsearch1.best_estimator_.predict(X_test)\n",
    "\n",
    "\n",
    "test_f1 = f1_score(y_test, preds)\n",
    "test_acc = accuracy_score(y_test, preds)\n",
    "\n",
    "print(\"Accuracy: %f\" % (test_acc))\n",
    "print(\"F1: %f\" % (test_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import plot_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a251f66d0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# plot feature importance\n",
    "plot_importance(alg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Fare': 231,\n",
       " 'Parch': 91,\n",
       " 'male': 42,\n",
       " 'Pclass': 81,\n",
       " 'SibSp': 86,\n",
       " 'S': 43,\n",
       " 'Q': 37,\n",
       " 'youngin': 27,\n",
       " 'Age': 150}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alg.get_booster().get_fscore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cloud Computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maybe take some time to look it up, could be useful for final project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_clf = xgb.train(params=params, dtrain=data_dmatrix, num_boost_round=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle list object\n",
    " \n",
    "model_pickle_path = 'xg_boost_model.pkl'\n",
    "\n",
    "# Create an variable to pickle and open it in write mode\n",
    "model_pickle = open(model_pickle_path, 'wb')\n",
    "pickle.dump(gsearch1.best_estimator_, model_pickle)\n",
    "model_pickle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded XGboost model ::  XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=0.5, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "              min_child_weight=3, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=500, n_jobs=0, num_parallel_tree=1,\n",
      "              objective='binary:logistic', random_state=0, reg_alpha=0,\n",
      "              reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n"
     ]
    }
   ],
   "source": [
    "# Loading the saved XGboost model pickle\n",
    "xgboost_model_pkl = open(model_pickle_path, 'rb')\n",
    "xgboost_model = pickle.load(xgboost_model_pkl)\n",
    "print(\"Loaded XGboost model :: \", xgboost_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_params = [\n",
    "    (max_depth, min_child_weight)\n",
    "    for max_depth in range(9,12)\n",
    "    for min_child_weight in range(5,8)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9, 5), (9, 6), (9, 7), (10, 5), (10, 6), (10, 7), (11, 5), (11, 6), (11, 7)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
